{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b0d574-6e31-4523-a440-de12af8fbb36",
   "metadata": {},
   "source": [
    "# Text pre-processing\n",
    "\n",
    "This collects various pre-processing steps for unstructured text in a typical NLP pipeline. All of these made sense in 2018 and likely still do in many cases.\n",
    "\n",
    "## Resources\n",
    " - [Understanding Feature Engineering - 3 - Unstructured text data](./NLP/pdfs/Understanding%20Feature%20Engineering%203%20-%20text%20data%20-%20Dipanjan%20Sarkar.pdf)\n",
    " - [Practical Machine Learning with Python - Text normalization](https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/Text%20Normalization%20Demo.ipynb)\n",
    "\n",
    "\n",
    "## Loading up text from a corpus\n",
    "\n",
    ">  Corpus _a collection of text documents usually belonging to a limited set of subjects_\n",
    "\n",
    "## Import necessary dependencies\n",
    " - pandas\n",
    " - numpy\n",
    " - nltk\n",
    " - spacy\n",
    " - BeautifulSoup (bs4 package)\n",
    " - unicodedata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed95eb95-3d6b-4d58-a14f-67201fcaf1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the models to use if it is the first time using it.\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb9af538-3296-478e-9265-63c82f9ccf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download stop words if not installed already. \n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "stops_set = set(stops)\n",
    "stops_set.remove('no')   # ??\n",
    "stops_set.remove('not')  # ??\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "# old: nlp = spacy.load('en', parse = False, tag=False, entity=False)\n",
    "# Download model first using\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext nb_js_diagrammers\n",
    "import iplantuml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b405ce9-a457-4e7e-88c8-eb31f04a5f62",
   "metadata": {},
   "source": [
    "# About Text pre-processing\n",
    "\n",
    "There are many ways of cleaning up data so it can be better input to traditional methods meant for regression analysis.\n",
    " - All of them have specific reasons for existing\n",
    "   - they remove things that do not add value to the model\n",
    "   - they remove things that can give you a bad model\n",
    " - However, based on the specific problem you are trying to achieve, all of them might warrant a closer look to see if they hurt or help.\n",
    "\n",
    "The following are copied from Sarkar's jupyter notebook (link above) except I an running them with example data where possible. Dig deeper into these as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc5f69f7-1af0-4bef-a889-747b9e4db6c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing output for /home/vamsi/bitbucket/hillops/docs/AI/NLP/91cc6af8-b3cd-4dae-bfea-b2b4074d80c7.uml to 91cc6af8-b3cd-4dae-bfea-b2b4074d80c7.svg\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" contentStyleType=\"text/css\" height=\"851px\" preserveAspectRatio=\"none\" style=\"width:1232px;height:851px;background:#FFFFFF;\" version=\"1.1\" viewBox=\"0 0 1232 851\" width=\"1232px\" zoomAndPan=\"magnify\"><defs/><g><rect fill=\"#FFFF00\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"178\" x=\"10\" y=\"406.75\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"142\" x=\"20\" y=\"429.7451\">Text Pre Processing</text><rect fill=\"#F1F1F1\" height=\"52.5938\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"363\" x=\"238\" y=\"20\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"4\" x=\"248\" y=\"42.9951\">¬†</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"105\" x=\"252\" y=\"42.9951\">Remove tags</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"230\" x=\"361\" y=\"42.9951\">- Removes unnecessary content</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"143\" x=\"248\" y=\"59.292\">or formatting which</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"139\" x=\"395\" y=\"59.292\">doesnt add value</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"74\" x=\"654\" y=\"32.8467\">HTML tags</text><path d=\"M601,46.2969 L611,46.2969 C626,46.2969 626,28 641,28 L651,28 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"65\" x=\"654\" y=\"51.1436\">XML tags</text><path d=\"M601,46.2969 L611,46.2969 C626,46.2969 626,46.2969 641,46.2969 L651,46.2969 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"231\" x=\"654\" y=\"69.4404\">BeautifulSoup lib helps with this</text><path d=\"M601,46.2969 L611,46.2969 C626,46.2969 626,64.5938 641,64.5938 L651,64.5938 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><path d=\"M188,424.8984 L198,424.8984 C213,424.8984 213,46.2969 228,46.2969 L238,46.2969 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"68.8906\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"406\" x=\"238\" y=\"92.5938\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"4\" x=\"248\" y=\"115.5889\">¬†</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"235\" x=\"252\" y=\"115.5889\">Remove accented characters</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"143\" x=\"491\" y=\"115.5889\">- removes accented</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"359\" x=\"252\" y=\"131.8857\">(foreign lang?) characters and converts to nearest</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"366\" x=\"252\" y=\"148.1826\">english ones. Remove their influence on the model</text><path d=\"M188,424.8984 L198,424.8984 C213,424.8984 213,127.0391 228,127.0391 L238,127.0391 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"52.5938\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"372\" x=\"238\" y=\"181.4844\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"4\" x=\"248\" y=\"204.4795\">¬†</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"159\" x=\"252\" y=\"204.4795\">Expand contraction</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"185\" x=\"415\" y=\"204.4795\">canonicalize the language</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"185\" x=\"248\" y=\"220.7764\">by removing contractions</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"105\" x=\"663\" y=\"203.4795\">don't ‚û°Ô∏è do not</text><path d=\"M610,207.7813 L620,207.7813 C635,207.7813 635,198.6328 650,198.6328 L660,198.6328 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"86\" x=\"663\" y=\"221.7764\">I'd ‚û°Ô∏è I would</text><path d=\"M610,207.7813 L620,207.7813 C635,207.7813 635,216.9297 650,216.9297 L660,216.9297 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><path d=\"M188,424.8984 L198,424.8984 C213,424.8984 213,207.7813 228,207.7813 L238,207.7813 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"68.8906\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"474\" x=\"238\" y=\"254.0781\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"4\" x=\"248\" y=\"277.0732\">¬†</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"218\" x=\"252\" y=\"277.0732\">Remove special characters</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"195\" x=\"474\" y=\"277.0732\">typically non-alphanumeric</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"454\" x=\"248\" y=\"293.3701\">characters often add to the noise. Simple regex based removal</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"47\" x=\"248\" y=\"309.667\">usually</text><path d=\"M188,424.8984 L198,424.8984 C213,424.8984 213,288.5234 228,288.5234 L238,288.5234 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"255\" x=\"238\" y=\"436.3047\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"82\" x=\"248\" y=\"459.2998\">Stemming</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"26\" x=\"334\" y=\"459.2998\">and</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"119\" x=\"364\" y=\"459.2998\">lemmatization</text><rect fill=\"#F1F1F1\" height=\"101.4844\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"348\" x=\"543\" y=\"342.9688\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"4\" x=\"553\" y=\"365.9639\">¬†</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"82\" x=\"557\" y=\"365.9639\">Stemming</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"171\" x=\"643\" y=\"365.9639\">reffers to obtaining the</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"314\" x=\"553\" y=\"382.2607\">base form of a word (it's stem) by removing</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"168\" x=\"553\" y=\"398.5576\">prefixes and/or suffixes</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"4\" x=\"553\" y=\"414.8545\">¬†</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"328\" x=\"553\" y=\"431.1514\">üëâ Stem may or maynot be a dictionary word!</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"276\" x=\"944\" y=\"398.5576\">Watches, Watched, Watching ‚û°Ô∏è Watch</text><path d=\"M891,393.7109 L901,393.7109 C916,393.7109 916,393.7109 931,393.7109 L941,393.7109 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><path d=\"M493,454.4531 L503,454.4531 C518,454.4531 518,393.7109 533,393.7109 L543,393.7109 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"101.4844\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"370\" x=\"543\" y=\"464.4531\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"4\" x=\"553\" y=\"487.4482\">¬†</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"123\" x=\"557\" y=\"487.4482\">Lemmatization</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"161\" x=\"684\" y=\"487.4482\">is similar to Stemming</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"237\" x=\"553\" y=\"503.7451\">and refers to the invariant as the</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"80\" x=\"794\" y=\"503.7451\">root word</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"276\" x=\"553\" y=\"520.042\">which can be different from the stem.</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"4\" x=\"553\" y=\"536.3389\">¬†</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"123\" x=\"553\" y=\"552.6357\">üëâ The root word</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"223\" x=\"680\" y=\"552.6357\">is always a dictionary word</text><path d=\"M493,454.4531 L503,454.4531 C518,454.4531 518,515.1953 533,515.1953 L543,515.1953 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><path d=\"M188,424.8984 L198,424.8984 C213,424.8984 213,454.4531 228,454.4531 L238,454.4531 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"134.0781\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"474\" x=\"238\" y=\"585.9375\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"4\" x=\"248\" y=\"608.9326\">¬†</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"170\" x=\"252\" y=\"608.9326\">Removing stopwords</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"208\" x=\"426\" y=\"608.9326\">- Words that have little or no</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"85\" x=\"248\" y=\"625.2295\">significance</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"202\" x=\"337\" y=\"625.2295\">to constructing features</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"130\" x=\"543\" y=\"625.2295\">are known as stop</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"446\" x=\"248\" y=\"641.5264\">words. These usually have the maximum frequency of occuring</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"334\" x=\"248\" y=\"657.8232\">if you do a simple word freq count on a corpus.</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"4\" x=\"248\" y=\"674.1201\">¬†</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"450\" x=\"248\" y=\"690.417\">Their removal de-emphasiszes them to the model which would</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"454\" x=\"248\" y=\"706.7139\">otherwise overindex on their importance because of occurance</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"94\" x=\"765\" y=\"639.5264\">a, an, the etc</text><path d=\"M712,652.9766 L722,652.9766 C737,652.9766 737,634.6797 752,634.6797 L762,634.6797 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"128\" x=\"765\" y=\"657.8232\">universal list from</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-style=\"italic\" lengthAdjust=\"spacing\" textLength=\"26\" x=\"897\" y=\"657.8232\">nltk</text><path d=\"M712,652.9766 L722,652.9766 C737,652.9766 737,652.9766 752,652.9766 L762,652.9766 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"273\" x=\"765\" y=\"676.1201\">domain specific stop words as needed</text><path d=\"M712,652.9766 L722,652.9766 C737,652.9766 737,671.2734 752,671.2734 L762,671.2734 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><path d=\"M188,424.8984 L198,424.8984 C213,424.8984 213,652.9766 228,652.9766 L238,652.9766 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"12.5\" ry=\"12.5\" style=\"stroke:#181818;stroke-width:1.5;\" width=\"125\" x=\"238\" y=\"766.7578\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacing\" textLength=\"105\" x=\"248\" y=\"789.7529\">Misc cleanup</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"92\" x=\"416\" y=\"744.0107\">tokenization</text><path d=\"M363,784.9063 L373,784.9063 C388,784.9063 388,739.1641 403,739.1641 L413,739.1641 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"183\" x=\"416\" y=\"762.3076\">remove extra whitespace</text><path d=\"M363,784.9063 L373,784.9063 C388,784.9063 388,757.4609 403,757.4609 L413,757.4609 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"88\" x=\"416\" y=\"780.6045\">lower casing</text><path d=\"M363,784.9063 L373,784.9063 C388,784.9063 388,775.7578 403,775.7578 L413,775.7578 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"135\" x=\"416\" y=\"798.9014\">spelling correction</text><path d=\"M363,784.9063 L373,784.9063 C388,784.9063 388,794.0547 403,794.0547 L413,794.0547 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"209\" x=\"416\" y=\"817.1982\">grammatical error correction</text><path d=\"M363,784.9063 L373,784.9063 C388,784.9063 388,812.3516 403,812.3516 L413,812.3516 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"239\" x=\"416\" y=\"835.4951\">remove repetaed characters etc.</text><path d=\"M363,784.9063 L373,784.9063 C388,784.9063 388,830.6484 403,830.6484 L413,830.6484 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><path d=\"M188,424.8984 L198,424.8984 C213,424.8984 213,784.9063 228,784.9063 L238,784.9063 \" fill=\"none\" style=\"stroke:#181818;stroke-width:1.0;\"/><!--SRC=[PLLjRXn54FtVfoZ43ziZVv048N1-40K2IMHBLjPEa12AQdjgTrlkwHwwUtQzlW6_41S0SGlEmmNW2BpgcGLliBJoz5VLgzVlLJ_BXLFfN6Wxxatp_STRyJvkVg1hkIjqbGI_Q2Ld5vQ4FzCqvzGqRwIBQw72ozmqT4BZEDCGWkXkJbkoCHG9XKnCj8Yfur8qo6Rbx0eXsYWPgzosj6O_IDCyHV3cFRsylhoeaSVXTtkZvy93SOl1p-BGatTpMedlCsrSML5PkMps4RArm20jsHKdjaNI23XDW7VhXWw1KTmoaEUm_FBemG7Yq6erQqabKuaKX9FaOaZ2qhkyeXWadzAEb9MuH2uiK5ImWaMTeYwsuf_kuBsuwwUeHREu603BSeZ1MVRkleQfK0PUYfblHyHAu8CpUQAiZU6Wq9-__lxN7pzZG26MSU7LGRkRVaMREFZs0tvoBzQntwUdR7j5uRS854xOzoiEGoV9MVE0bRZ0zTRx0oMADaINvPHchkkz0EzIxcZE6UHNxEpDa0SD2nQ00Y1cHJfeRuc1ij4q7cCkxfv7HhI8y_rzIHOBpOwSSLxO1QL4injDLOK6PCIeDhLqwCf1fepZH_IGGzCZZhkJUhTd46WU5dLipD-__V8JQKBgU4jOmZ_mINE8YLfNULTvQ_oFAl_W-HqNkv9yF7sqquUcckYlOxChw6A_Jd80wJhdEMbXktB9A2j0EXMiPRgmvkHOdPFhHDEa27WA1uIDxeAG56_hb2lru2B5hkvMAawd8gyn_lSiuW04-mrly_yBLNFkAKVHvH9xNHmTzKw_a8CBhHZAyguKo004XaWcmrbk0LMf8ve6nK36kQJ1rgQm42u3FAL2GC-v3N4Jj41DGQOcELMykE59GsEIQYo-SztGeKRvKHstrVkFrWu9aOrRq3OEQWfMYbMPjLpT3Gm3o8516P-f7pAekQxsdHHBhPn81_rdb- -b4jxjRg8QUcfdrLheT9XF6xY0u2saR-60qVxQ4Re-fb89c8lbGNTDG7MojWP8YO-XoUEQGOeTvuRW42y33zhDT9jdPy6NsxEpNGVeu8NHpg1v9AxofZG6aLRQoNINBbkoNk3fVhJOUvHqAs6IupWpDaZ2Av1OYooIUxOoBkA5a0ILwQimpY2nzsDtIadiVt6MYQlKqKz8Ke8SFjmnPKhII-6zJgq4u4QU2LwdwOKoL-Y9vGPlWXA26FJezF4dZv-SVZ77wKU7Ds7KZerTxt3NnNLoP0w_lRgW78TarHIGdPiF2k38lEOrqnke0FlEIKU7rvT7D7kncwGNOUrI31riP5w_lHmtqSjOPdqiTVDddvuyTuLcae29tbwQhsN1WoyuQcCBKixfvlgRay_DnTJEp_5mcAzKUscBjTa_]--></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%plantuml\n",
    "\n",
    "@startmindmap\n",
    "*[#yellow] Text Pre Processing    \n",
    "**: **Remove tags** - Removes unnecessary content \n",
    "or formatting which **doesnt add value**;\n",
    "***_ HTML tags\n",
    "***_ XML tags\n",
    "***_ BeautifulSoup lib helps with this\n",
    "\n",
    "**: **Remove accented characters** - removes accented\n",
    " (foreign lang?) characters and converts to nearest\n",
    " english ones. Remove their influence on the model;\n",
    "\n",
    "**: **Expand contraction** canonicalize the language\n",
    "by removing contractions;\n",
    "***_ don't ‚û°Ô∏è do not\n",
    "***_ I'd ‚û°Ô∏è I would\n",
    "\n",
    "**: **Remove special characters** typically non-alphanumeric\n",
    "characters often add to the noise. Simple regex based removal\n",
    "usually;\n",
    "\n",
    "\n",
    "** **Stemming** and **lemmatization**\n",
    "***: **Stemming** reffers to obtaining the \n",
    "base form of a word (it's stem) by removing \n",
    "prefixes and/or suffixes\n",
    "\n",
    "üëâ Stem may or maynot be a dictionary word!;\n",
    "****_ Watches, Watched, Watching ‚û°Ô∏è Watch\n",
    "\n",
    "***: **Lemmatization** is similar to Stemming \n",
    "and refers to the invariant as the **root word**\n",
    "which can be different from the stem.\n",
    "\n",
    "üëâ The root word **is always a dictionary word**;\n",
    "\n",
    "\n",
    "**: **Removing stopwords** - Words that have little or no \n",
    "significance **to constructing features** are known as stop \n",
    "words. These usually have the maximum frequency of occuring\n",
    "if you do a simple word freq count on a corpus.\n",
    "\n",
    "Their removal de-emphasiszes them to the model which would \n",
    "otherwise overindex on their importance because of occurance;\n",
    "\n",
    "***_ a, an, the etc\n",
    "***_ universal list from //nltk//\n",
    "***_ domain specific stop words as needed\n",
    "\n",
    "** **Misc cleanup**\n",
    "***_ tokenization\n",
    "***_ remove extra whitespace\n",
    "***_ lower casing\n",
    "***_ spelling correction\n",
    "***_ grammatical error correction\n",
    "***_ remove repetaed characters etc.\n",
    "\n",
    "@endmindmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9411f6-1cac-4ddf-81a6-a987708a8f31",
   "metadata": {},
   "source": [
    "# Cleanup text - strip html\n",
    "\n",
    "The idea here is to remove unnecessary content which doesn't add value to the problem we are modeling. Those tags take on semantic importance when left as part of the input which we want to avoid by this step.\n",
    "\n",
    " - It is formatting sugar\n",
    " - Don't want regression model to learn about the tags themselves but\n",
    "\n",
    "`BeautifulSoup` can strip XML tags out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08ee746d-f0ac-44e8-ae95-c82b4c50df21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "My First HTML \n",
      "\n",
      "\n",
      " Introduction \n",
      " Have you ever read the source code of a html page? This is how to get back to the course page: ENC2036. \n",
      " Contents of the Page \n",
      " Anything you can say about the page.....\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html_text = \"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "  <title>My First HTML </title>\n",
    "\n",
    "  </head>\n",
    "  \n",
    "  <body>\n",
    "    <h1> Introduction </h1>\n",
    "    <p> Have you ever read the source code of a html page? This is how to get back to the course page: <a href=\"https://alvinntnu.github.io/NTNU_ENC2036_LECTURES/\", target=\"_blank\">ENC2036</a>. </p>\n",
    "    <h1> Contents of the Page </h1>\n",
    "    <p> Anything you can say about the page.....</p>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "print(strip_html_tags(html_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02582f21-6f99-4896-b1bf-3fd015baead7",
   "metadata": {},
   "source": [
    "# Remove accented characters\n",
    "\n",
    "Removing these is motivated by the same reasons as above. \n",
    " - These are mostly foreign words\n",
    " - Do not want the model to integrate them when we are looking for an english language model\n",
    " - Hoever, revisit as the model needs change.\n",
    "\n",
    "I used an online lorem-ipsum generator to get accented char samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c19c934c-65b9-4686-869f-73083a72715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem ipsum dolor sit amet, assidua bona fastidium forensibus meo putem. Aliquod comparaverit corrigere fingi, importari iudicatum labore libenter, moderatio plane sensu. Amicitiam audaces certae legimus senserit. Eoque numquid quantaque sociis videntur. \n",
      "Allicit animal bello deleniti erga, existimare imitarentur netus verum. Depravata desistemus disputando formidinum liberos metrodorus metum modum poterimus. Animal consequatur excruciant suaviter. Chrysippe dicam effici eosdem eveniet, graecum impediente nam obruamus ponti possunt reliqui tali transferam ullius. Cognosci facile labitur minimum necessitatibus. Affecti dedocere fautrices logikh, meminerunt porta. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "accented_words = \"\"\"Lor√©m ipsum d√≥l√≥r sit amet, assidua bona fastidium for√©nsibus meo put√©m. Aliquod comparaverit corrigere fingi, imp√≥rtari iudicatum labore libenter, moderatio plane sensu. Amicitiam audaces c√©rta√© legimus senserit. Eoque numquid quantaque sociis videntur. \n",
    "Allicit animal b√©llo deleniti erga, existimare imitarentur netus verum. Depravata desistemus disputando formidinum liberos metrodorus metum modum poterimus. Animal cons√©quatur excruciant su√°viter. Chrysippe dic√°m effici eosdem eveniet, graecum impediente n√°m √≥bruamus ponti p√≥ssunt reliqui tali transferam ullius. Cognosci facile labitur minimum necessitatibus. Affecti dedocere fautric√©s logikh, m√©min√©runt porta. \n",
    "\"\"\"\n",
    "\n",
    "def remove_accented_characters(text):\n",
    "    return unicodedata.normalize(\n",
    "        'NFKD', text\n",
    "    ).encode(\n",
    "        'ascii', 'ignore'\n",
    "    ).decode('utf-8', 'ignore')\n",
    "\n",
    "print(remove_accented_characters(accented_words))                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d61ac-a5a6-407e-90cb-49c0d51a905f",
   "metadata": {},
   "source": [
    "# Expand Contractions\n",
    "\n",
    "This is done to canonicalize/normalize the text. We replace all contractions by their expanded forms. For e.g.,\n",
    " - don't ‚û°Ô∏è do not\n",
    " - I'd ‚û°Ô∏è I would\n",
    " - and so on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0353c537-6990-49ba-b399-1569ce451563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You shall not do this. I would be very disappointed if you did.\n"
     ]
    }
   ],
   "source": [
    "def expand_contractions(text, contraction_mapping = contractions.contractions_dict):\n",
    "    \n",
    "    # Build a complex pattern from the ORd keys of the contractions dict\n",
    "    contractions_pat = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                  flags = re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "\n",
    "        # Don't like this. Should simply pre-process the map to have all keys LCd\n",
    "        expanded_contraction = contraction_mapping.get(match) \\\n",
    "            if   contraction_mapping.get(match) \\\n",
    "            else contraction_mapping.get(match.lower())\n",
    "        \n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pat.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "test_str = \"\"\"You shan't do this. I'd be very disappointed if you did.\"\"\"\n",
    "print(expand_contractions(test_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8917b-a64b-4b6c-b053-79c151424d7b",
   "metadata": {},
   "source": [
    "# Removing Special Characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51cbc8fc-db80-4f7a-91be-53186cea4724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f01eb4-806b-4c85-9ca9-9c0ce0d5c4eb",
   "metadata": {},
   "source": [
    "# Lemmatizing text\n",
    "\n",
    "Note that while Stemming was also mentioned earlier, these days looks like only lemmatization is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6d23c0a-5110-43a9-9d53-83ffc8d622b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: nlp = spacy.load('en', parse = False, tag=False, entity=False)\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b76bc7-2672-4237-b9e3-6fd89141618a",
   "metadata": {},
   "source": [
    "# Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "843f07d7-765f-4edc-bd21-5e3375ada807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work play makes jack dull boy. work play makes jack dull boy .\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stops_set]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stops_set]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "print(remove_stopwords(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93433f8-4a06-47e8-aca3-5542dad03fd7",
   "metadata": {},
   "source": [
    "# Normalize text corpus - tying it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9f3ca00-46a7-4d12-9b25-c5c52e61af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_characters(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters    \n",
    "        if special_char_removal:\n",
    "            doc = remove_special_characters(doc)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71afc730-8dd8-421b-8164-804ec430af49",
   "metadata": {},
   "source": [
    "# Sample demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "505eafc9-21c1-41ce-9a7f-b6e2d5c63523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<p>H√©llo! H√©llo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \\n              It's an amazing language which can be used for Scripting, Web development,\\r\\n\\r\\n\\n              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\\n              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\\n              got a headstart!</p>\\n           \""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = \"\"\"<p>H√©llo! H√©llo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
    "              It's an amazing language which can be used for Scripting, Web development,\\r\\n\\r\\n\n",
    "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
    "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
    "              got a headstart!</p>\n",
    "           \"\"\"\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01da3dbf-97e6-4134-a042-ca2d9459885b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Hello can you hear me I just heard about Python it is an amazing language which can be used for Scripting Web development Information Retrieval Natural Language Processing Machine Learning Artificial Intelligence What are you waiting for Go and get started he is learning she is learning they have already got a headstart ']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_corpus([document], text_lemmatization=False, stopword_removal=False, text_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "103aba82-f6b8-423d-888e-f2fd07b6035a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello hello hear I I hear python amazing language use scripting web development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_corpus([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6248450-0d6e-4f34-93f0-37812d620b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
