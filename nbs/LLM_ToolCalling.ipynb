{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Function Calling - OpenAI\n",
    "\n",
    "This demonstrates intermediate function calling on LLMs: geared toward `OpenAI` but works identically on Groq which has an OpenAI compatible API. Other LLM providers should be similar.\n",
    "\n",
    "The hello-world of tool-calling, _get_weather_ is described in the official [OpenAI docs](https://platform.openai.com/docs/guides/function-calling). This version demonstrates an enhancement on that by utilizing pydantic classes to\n",
    " - Automate generation of the JSON schema required for tools\n",
    " - Automate deserialization of the JSON args supplied by OpenAI\n",
    " - Simplify implementation of tooling for ReAct and any other LLM use case.\n",
    " - Similar infra is also used for obtaining structured outputs from OpenAI and others.\n",
    "\n",
    "There are two scenarios demonstrated here\n",
    " - `What is the weather in XX` which validates basic tool calling\n",
    " - `Increase Temperature by YY` which tests if the LLM can first call `get_temperature`, do some math and then call `set_temperature`.\n",
    "\n",
    "\n",
    "## Setup Basic Environment\n",
    "\n",
    "> Normally, these would go into a python module and I would include it in my \n",
    "module search path. However, when executed in colab directly from github. It will  require that I put my lib code in a repo, clone said repo into colab environment and then add it to my path. Something for later!\n",
    "\n",
    " - Logging\n",
    " - Colab environment\n",
    " - OpenAI functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:33:55 DEBUG:Checking if OPENAI_API_KEY is available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# If you want to log OpenAI's python library itself, also set the log level for this\n",
    "# normally, limit this to warning/error and keep your own logging at debug levels.\n",
    "# If this doesn't work right away, restart the kernel after changing the log-level\n",
    "os.environ[\"OPENAI_LOG\"]=\"error\"\n",
    "\n",
    "# Setup logging \n",
    "# Note that module needs to be reloaded for our config to take as Jupyter already configures it\n",
    "# which makes all future configs no-ops.\n",
    "from importlib import reload\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', \n",
    "                    level=logging.ERROR, \n",
    "                    datefmt='%I:%M:%S')\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "# Utility methods to visually separate output from logs\n",
    "# displaying HTML and Markdown responses\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Enhance with more Html (fg-color, font, etc) as needed but title is usually a good starting point.\n",
    "def colorBox(txt, title=None):\n",
    "    if title is not None:\n",
    "        txt = f\"<b>{title}</b><br><hr><br>{txt}\"\n",
    "\n",
    "    display(HTML(f\"<div style='border-radius:15px;padding:15px;background-color:pink;color:black;'>{txt}</div>\"))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Configure for Colab\n",
    "# You could do one of the two.\n",
    "# Either paste your OpenAI Key here or put it in secrets\n",
    "#-------------------------------------------------------------\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  from google.colab import userdata\n",
    "  logging.debug(\"Tryign to fetch OPENAI_API_KEY from your secrets. Remember to make it available to this notebook\")\n",
    "  os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Finally ensure you have the OpenAI key.\n",
    "logging.debug(\"Checking if OPENAI_API_KEY is available\")\n",
    "assert(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "# Seup OpenAI completion function\n",
    "#-------------------------------------------------------------\n",
    "import openai\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-4o-mini\", tools = None, temperature=0) -> str:\n",
    "    chat_history = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    response = get_response(chat_history=chat_history, \n",
    "                            model=model,\n",
    "                            tools = tools, \n",
    "                            temperature=temperature)    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_response(chat_history, model=\"gpt-4o-mini\", tools = None, temperature=0) -> str:\n",
    "    messages = chat_history\n",
    "    return openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        temperature=temperature)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pydantic to simplify schema creation\n",
    "\n",
    "What OpenAI _(and by extension, every other vendor who wants to be compatible)_ needs.\n",
    " - A subset of JSON schema: _they don't spell it out, but from experience, I found that_\n",
    "   - `ref` fields are not allowed. This means, schema references have to be resolved in-place. Thankfully a simple lib exists.\n",
    " - As much natural language description as possible\n",
    "   - each function parameter to have a useful name and description\n",
    "   - well thought out field names and descriptions\n",
    "\n",
    "In PyDantic terms\n",
    " - To use the PyDantic model more easily, optionally make it a `dataclass`\n",
    " - Use `Field` to supply per-field description\n",
    " - use `jsonref.resolve_refs` to resolve any schema refs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import inspect\n",
    "import jsonref\n",
    "from typing import TypeVar\n",
    "\n",
    "F = TypeVar('F')\n",
    "\n",
    "def getToolJsonSchema(f:F) -> json:\n",
    "    \"\"\"\n",
    "    f: A function of the form 'func(arg:BaseModel)` | `func()`\n",
    "      Max of 1 argument and it should be a class deriving from PyDantic BaseModel  \n",
    "\n",
    "    Eg.\n",
    "\n",
    "    @dataclass\n",
    "    class GetWeather(BaseModel):        \n",
    "        location : str = Field(description=\"City and country e.g. San Jose, USA\")\n",
    "\n",
    "    def get_weather(args: GetWeather) -> float:\n",
    "        '''\n",
    "        Get current temperature for a given location.\n",
    "        '''\n",
    "        return 10    \n",
    "\n",
    "    And use this thus:\n",
    "    \n",
    "        tool_schema = getToolJsonSchema(get_weather)\n",
    "    \"\"\"    \n",
    "    if inspect.isfunction(f):\n",
    "        sig = inspect.signature(f)\n",
    "        params = list(sig.parameters.items())\n",
    "\n",
    "        if len(params) > 1:\n",
    "            # more than 1 args is an error for us!\n",
    "            raise TypeError(f\"{str(f)} should have just max of 1 arguments\")\n",
    "        elif len(params):            \n",
    "            # 1 args case\n",
    "            p_ann   = params[0][1].annotation            \n",
    "            p_class = p_ann.__class__\n",
    "\n",
    "            if \"pydantic._internal._model_construction.ModelMetaclass\" not in str(p_class):\n",
    "                raise TypeError(f\"{str(f)} must derive from Pydantic's BaseModel\")\n",
    "\n",
    "            # exec the class method to generate the json schema\n",
    "            arg_json_schema = getattr(p_ann, 'model_json_schema').__call__()\n",
    "            arg_json_schema = jsonref.replace_refs(arg_json_schema)\n",
    "\n",
    "            # üëâ OpenAI enforces that \"additionalProperties\" : False is set!\n",
    "            arg_json_schema[\"additionalProperties\"] = False\n",
    "                        \n",
    "            # Now generate the schema for the full function\n",
    "            # See https://platform.openai.com/docs/guides/function-calling\n",
    "            func_schema = {\n",
    "                \"type\"       : \"function\",\n",
    "                \"function\"   : {\n",
    "                    \"name\"       : f.__name__,\n",
    "                    \"description\": f.__doc__.strip(),\n",
    "                    \"parameters\" : arg_json_schema,\n",
    "                    \"strict\"     : True\n",
    "                }\n",
    "            }            \n",
    "        else:\n",
    "            # 0 args case            \n",
    "            # See https://platform.openai.com/docs/guides/function-calling\n",
    "            func_schema = {\n",
    "                \"type\"       : \"function\",\n",
    "                \"function\"   : {\n",
    "                    \"name\"       : f.__name__,\n",
    "                    \"description\": f.__doc__.strip()                    \n",
    "                }\n",
    "            }        \n",
    "        \n",
    "        return func_schema\n",
    "    else:\n",
    "        raise TypeError(f\"{str(f)} should be a python function\")    \n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# Tool execution infrastructure\n",
    "#------------------------------------------------------------------------\n",
    "# Name -> { deserializer, executor} map\n",
    "import json\n",
    "from pydantic import ValidationError\n",
    "\n",
    "class ToolExecutor:\n",
    "    # tool_arg_deserializer(str) -> Obj. Expect pydantic's ValidationError\n",
    "    # tool_func(obj)             -> str\n",
    "    def __init__(self, name:str,                  \n",
    "                 tool_arg_deserializer, \n",
    "                 tool_func):\n",
    "        self.name = name\n",
    "        self.tool_arg_deserializer = tool_arg_deserializer\n",
    "        self.tool_func = tool_func\n",
    "    \n",
    "    def exec(self, json_arg: str) -> str:\n",
    "        if self.tool_arg_deserializer:\n",
    "            try:\n",
    "                logging.debug(f\"Attempting to deserialize {json_arg} for tool: {self.name}\")\n",
    "\n",
    "                arg_obj = self.tool_arg_deserializer(json_arg)\n",
    "                logging.debug(f\"‚úîÔ∏è deserialized to {arg_obj}. Calling function\")\n",
    "\n",
    "                func_result = self.tool_func(arg_obj)\n",
    "                logging.debug(f\"‚úîÔ∏è function returned {func_result}\")\n",
    "\n",
    "                return func_result\n",
    "\n",
    "            except ValidationError as e:\n",
    "                logging.error(f\"JSON string {json_arg} is not valid for {self.name}:{e}\")\n",
    "        else:            \n",
    "            logging.debug(f\"Calling no-arg function: {self.name}\")            \n",
    "            assert(json_arg == '{}' or json_arg is None)\n",
    "            \n",
    "            func_result = self.tool_func()\n",
    "            logging.debug(f\"‚úîÔ∏è function returned {func_result}\")\n",
    "            return func_result\n",
    "\n",
    "    \n",
    "# {name : ToolExecutor}*\n",
    "tool_dict = {}\n",
    "        \n",
    "def exec_tool(name: str, args: str) -> str :\n",
    "    if not name in tool_dict:\n",
    "        raise KeyError(f\"Tool: {name} is not registered! Cannot call!\")\n",
    "    else:\n",
    "        return tool_dict[name].exec(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The chat loop with tools involved\n",
    "\n",
    "Unlike a one-shot prompt, when tools are used:\n",
    "\n",
    " - the LLM can respond with one or more `tool call`s instead of an `assistant response`. \n",
    " - We need to evaluate all the tool calls and respond. \n",
    " - This is continued till the LLM responsds with an assistant response \n",
    " - Then we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat_loop(prompt:str, tools):\n",
    "    \"\"\"\n",
    "    Runs a chat loop with an initial prompt and supplied tools\n",
    "    Resolves all tool_calls made till a final assistant response is provided\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    chat_history = [\n",
    "        {\n",
    "            \"role\" : \"system\",\n",
    "            \"content\" : \"You are a helpful assistant that uses the supplied tools to response to the user's questions.\"\n",
    "        }]\n",
    "\n",
    "    # Run the loop\n",
    "    msgs = [{\n",
    "        \"role\":\"user\", \n",
    "        \"content\": prompt}]\n",
    "    \n",
    "    while len(msgs):\n",
    "        chat_history.extend(msgs)\n",
    "        msgs = []\n",
    "\n",
    "        response = get_response(\n",
    "            chat_history=chat_history,\n",
    "            tools = tools)\n",
    "\n",
    "        # tool-call\n",
    "        # Note: The OpenAI example is outdated\n",
    "        # tool_calls is not longer a JSON object but an array of \n",
    "        # `ChatCompletionMessageToolCall` objects\n",
    "        if response.choices[0].message.tool_calls:\n",
    "\n",
    "            # The tool-call set needs to be added back to the chat_history\n",
    "            msgs.append(response.choices[0].message)\n",
    "\n",
    "            # Process all the tool calls\n",
    "            for tool_call in response.choices[0].message.tool_calls:\n",
    "                logging.debug(f\"Executing tool_call: {tool_call}\")            \n",
    "\n",
    "                colorBox(f\"{tool_call.function.name}({tool_call.function.arguments})\", title=\"LLM Tool Call\")\n",
    "                tool_result = exec_tool(\n",
    "                    tool_call.function.name,\n",
    "                    tool_call.function.arguments)\n",
    "                assert(isinstance(tool_result, str))\n",
    "\n",
    "                # along with it's response. The response will be linked to the tool_call's \n",
    "                # via the ID        \n",
    "                msgs.append({\n",
    "                    \"role\" : \"tool\",\n",
    "                    \"tool_call_id\" : tool_call.id,\n",
    "                    \"content\"      : tool_result\n",
    "                })\n",
    "        else:\n",
    "            # Assistant response\n",
    "            chat_response = response.choices[0].message.content\n",
    "            colorBox(chat_response, title=\"Final LLM Response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement OpenAI's get_weather example\n",
    "\n",
    "See https://platform.openai.com/docs/guides/function-calling\n",
    "\n",
    "The example is listed below\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for a given location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and country e.g. Bogot√°, Colombia\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"location\"\n",
    "            ],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}],\n",
    "    tools=tools\n",
    ")\n",
    "```\n",
    "\n",
    "My goal is to automate the creation of the json given a function. The previously created `getToolJsonSchema` handles the creation of the schema given a function: the limitation is that the function, if it has arguments is limited to just 1 and it must be a pydantic class.\n",
    "\n",
    "> üëâ Note that in a production scenario, one will enfore that all the fields and the function itself will have a reasonable descriptions. We need good natural language descriptions to allow the LLM to decide which tool to call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GetWeather(BaseModel):        \n",
    "    location : str = Field(description=\"City and country e.g. San Jose, USA\")\n",
    "\n",
    "def get_weather(args: GetWeather) -> float:\n",
    "    \"\"\"\n",
    "    Get current temperature for a given location.\n",
    "    \"\"\"\n",
    "    retval = \"10\"\n",
    "    logging.debug(f\"get_weather called with {args}. Returning hardcoded value {retval}\")\n",
    "    return retval\n",
    "\n",
    "# Register in the tool dictionary\n",
    "# Note that one could evolve this further to make the tool_dict the single \n",
    "# source and use it to the tool descriptions as well.\n",
    "tool_dict[\"get_weather\"] = ToolExecutor(\n",
    "    name = \"get_weather\",\n",
    "    tool_arg_deserializer = lambda json_str: GetWeather.model_validate_json(json_str),\n",
    "    tool_func = lambda gw: get_weather(gw)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test schema generation and execution!\n",
    "if 0:\n",
    "        # Verify that the JSON we get from out `get_weather` function matches the \n",
    "        # raw JSON used in the OpenAI example\n",
    "        print(json.dumps(\n",
    "                getToolJsonSchema(get_weather),\n",
    "                indent=4\n",
    "        ))\n",
    "\n",
    "        # Test using the OpenAI example's serialized JSON\n",
    "        exec_tool(\"get_weather\", \"{\\\"location\\\":\\\"Paris, France\\\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:25:37 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are a helpful assistant that uses the supplied tools to response to the user's questions.\"}, {'role': 'user', 'content': 'What is the weather in San Jose, USA?'}], 'model': 'gpt-4o-mini', 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'get_weather', 'description': 'Get current temperature for a given location.', 'parameters': {'properties': {'location': {'description': 'City and country e.g. San Jose, USA', 'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'GetWeather', 'type': 'object', 'additionalProperties': False}, 'strict': True}}]}}\n",
      "09:25:37 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "09:25:37 DEBUG:close.started\n",
      "09:25:37 DEBUG:close.complete\n",
      "09:25:37 DEBUG:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "09:25:37 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fbad7087800>\n",
      "09:25:37 DEBUG:start_tls.started ssl_context=<ssl.SSLContext object at 0x7fbad7741b50> server_hostname='api.openai.com' timeout=5.0\n",
      "09:25:37 DEBUG:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fbad7178eb0>\n",
      "09:25:37 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "09:25:37 DEBUG:send_request_headers.complete\n",
      "09:25:37 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "09:25:37 DEBUG:send_request_body.complete\n",
      "09:25:37 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "09:25:38 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 07 Mar 2025 05:25:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'394'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199948'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'15ms'), (b'x-request-id', b'req_4e7769b685b7d7f1a644e84ebc4f032a'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=60iH2Gy.xqdUWF10oD_K1cjWAavdNYjw_1NIenpKvRM-1741325140-1.0.1.1-6zcQ0ad9rL.DO2eqcqoyP4k6x5OWX66K9EuUbrKqCvfVOnJQanzCJmDgOC8vfocOEGSOSHdKtWjmYUyz63lGyRuGR6523udt22fWD1FymQE; path=/; expires=Fri, 07-Mar-25 05:55:40 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91c7ac6c4b582ea2-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "09:25:38 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "09:25:38 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "09:25:38 DEBUG:receive_response_body.complete\n",
      "09:25:38 DEBUG:response_closed.started\n",
      "09:25:38 DEBUG:response_closed.complete\n",
      "09:25:38 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 07 Mar 2025 05:25:40 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '394', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199948', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '15ms', 'x-request-id': 'req_4e7769b685b7d7f1a644e84ebc4f032a', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=60iH2Gy.xqdUWF10oD_K1cjWAavdNYjw_1NIenpKvRM-1741325140-1.0.1.1-6zcQ0ad9rL.DO2eqcqoyP4k6x5OWX66K9EuUbrKqCvfVOnJQanzCJmDgOC8vfocOEGSOSHdKtWjmYUyz63lGyRuGR6523udt22fWD1FymQE; path=/; expires=Fri, 07-Mar-25 05:55:40 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '91c7ac6c4b582ea2-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "09:25:38 DEBUG:request_id: req_4e7769b685b7d7f1a644e84ebc4f032a\n",
      "09:25:38 DEBUG:Executing tool_call: ChatCompletionMessageToolCall(id='call_tVbBvPZNGHyRLU8qJzbC2iRH', function=Function(arguments='{\"location\":\"San Jose, USA\"}', name='get_weather'), type='function')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;background-color:pink;color:black;'><b>LLM Tool Call</b><br><hr><br>get_weather({\"location\":\"San Jose, USA\"})</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:25:38 DEBUG:Attempting to deserialize {\"location\":\"San Jose, USA\"} for tool: get_weather\n",
      "09:25:38 DEBUG:‚úîÔ∏è deserialized to location='San Jose, USA'. Calling function\n",
      "09:25:38 DEBUG:get_weather called with location='San Jose, USA'. Returning hardcoded value 10\n",
      "09:25:38 DEBUG:‚úîÔ∏è function returned 10\n",
      "09:25:38 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are a helpful assistant that uses the supplied tools to response to the user's questions.\"}, {'role': 'user', 'content': 'What is the weather in San Jose, USA?'}, {'content': None, 'refusal': None, 'role': 'assistant', 'tool_calls': [{'id': 'call_tVbBvPZNGHyRLU8qJzbC2iRH', 'function': {'arguments': '{\"location\":\"San Jose, USA\"}', 'name': 'get_weather'}, 'type': 'function'}]}, {'role': 'tool', 'tool_call_id': 'call_tVbBvPZNGHyRLU8qJzbC2iRH', 'content': '10'}], 'model': 'gpt-4o-mini', 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'get_weather', 'description': 'Get current temperature for a given location.', 'parameters': {'properties': {'location': {'description': 'City and country e.g. San Jose, USA', 'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'GetWeather', 'type': 'object', 'additionalProperties': False}, 'strict': True}}]}}\n",
      "09:25:38 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "09:25:38 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "09:25:38 DEBUG:send_request_headers.complete\n",
      "09:25:38 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "09:25:38 DEBUG:send_request_body.complete\n",
      "09:25:38 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "09:25:39 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 07 Mar 2025 05:25:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'639'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'199947'), (b'x-ratelimit-reset-requests', b'16.765s'), (b'x-ratelimit-reset-tokens', b'15ms'), (b'x-request-id', b'req_62d23eba652288d0fa023b0a92d2a073'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91c7ac731b7a2ea2-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "09:25:39 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "09:25:39 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "09:25:39 DEBUG:receive_response_body.complete\n",
      "09:25:39 DEBUG:response_closed.started\n",
      "09:25:39 DEBUG:response_closed.complete\n",
      "09:25:39 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 07 Mar 2025 05:25:41 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '639', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '199947', 'x-ratelimit-reset-requests': '16.765s', 'x-ratelimit-reset-tokens': '15ms', 'x-request-id': 'req_62d23eba652288d0fa023b0a92d2a073', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '91c7ac731b7a2ea2-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "09:25:39 DEBUG:request_id: req_62d23eba652288d0fa023b0a92d2a073\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;background-color:pink;color:black;'><b>Final LLM Response</b><br><hr><br>The current temperature in San Jose, USA is 10¬∞C.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_chat_loop(\n",
    "    prompt=\"What is the weather in San Jose, USA?\",\n",
    "    tools = [getToolJsonSchema(get_weather)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement an IOT controller - Setting temperature\n",
    "\n",
    "> Demonstrates the ability of the LLM to sequence multiple tool calls.\n",
    "\n",
    "With the infra developed, this becomes quite simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SetTemperature(BaseModel):        \n",
    "    temp : float = Field(description=\"The temperature value in Fahrenheit to set the thermostat to\")\n",
    "\n",
    "def set_thermostat_temperature(args: SetTemperature) -> str:\n",
    "    \"\"\"\n",
    "    Sets the current temperature for a given location.\"\n",
    "    \"\"\"\n",
    "    logging.debug(f\"set_thermostat_temperature called with {args}\")\n",
    "    return \"\"\n",
    "\n",
    "def get_thermostat_temperature() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current temperature setting of the thermostat.\"\n",
    "    \"\"\"\n",
    "    retval = \"60\"\n",
    "    logging.debug(f\"get_thermostat_temperature called. Returning hardcoded {retval}\")\n",
    "    return retval\n",
    "\n",
    "# Register in the tool dictionary\n",
    "tool_dict.clear()\n",
    "tool_dict[\"set_thermostat_temperature\"] = ToolExecutor(\n",
    "    name = \"set_thermostat_temperature\",\n",
    "    tool_arg_deserializer = lambda json_str: SetTemperature.model_validate_json(json_str),\n",
    "    tool_func = lambda gw: set_thermostat_temperature(gw)\n",
    "    )\n",
    "\n",
    "tool_dict[\"get_thermostat_temperature\"] = ToolExecutor(\n",
    "    name = \"get_thermostat_temperature\",\n",
    "    tool_arg_deserializer = None,\n",
    "    tool_func = lambda: get_thermostat_temperature()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:32:24 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are a helpful assistant that uses the supplied tools to response to the user's questions.\"}, {'role': 'user', 'content': 'Increase the temperature by 10 degrees'}], 'model': 'gpt-4o-mini', 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'get_thermostat_temperature', 'description': 'Returns the current temperature setting of the thermostat.\"'}}, {'type': 'function', 'function': {'name': 'set_thermostat_temperature', 'description': 'Sets the current temperature for a given location.\"', 'parameters': {'properties': {'temp': {'description': 'The temperature value in Fahrenheit to set the thermostat to', 'title': 'Temp', 'type': 'number'}}, 'required': ['temp'], 'title': 'SetTemperature', 'type': 'object', 'additionalProperties': False}, 'strict': True}}]}}\n",
      "09:32:24 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "09:32:24 DEBUG:close.started\n",
      "09:32:24 DEBUG:close.complete\n",
      "09:32:24 DEBUG:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "09:32:24 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fbad7197c50>\n",
      "09:32:24 DEBUG:start_tls.started ssl_context=<ssl.SSLContext object at 0x7fbad7741b50> server_hostname='api.openai.com' timeout=5.0\n",
      "09:32:24 DEBUG:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fbad6f75450>\n",
      "09:32:24 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "09:32:24 DEBUG:send_request_headers.complete\n",
      "09:32:24 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "09:32:24 DEBUG:send_request_body.complete\n",
      "09:32:24 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "09:32:25 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 07 Mar 2025 05:32:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'391'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199948'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'15ms'), (b'x-request-id', b'req_e455802d9c797e30debf4233aac954e0'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91c7b65bcf512a9b-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "09:32:25 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "09:32:25 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "09:32:25 DEBUG:receive_response_body.complete\n",
      "09:32:25 DEBUG:response_closed.started\n",
      "09:32:25 DEBUG:response_closed.complete\n",
      "09:32:25 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 07 Mar 2025 05:32:27 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '391', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199948', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '15ms', 'x-request-id': 'req_e455802d9c797e30debf4233aac954e0', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '91c7b65bcf512a9b-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "09:32:25 DEBUG:request_id: req_e455802d9c797e30debf4233aac954e0\n",
      "09:32:25 DEBUG:Executing tool_call: ChatCompletionMessageToolCall(id='call_NRZFjLO5krfnRJ5l5sOGZh6c', function=Function(arguments='{}', name='get_thermostat_temperature'), type='function')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;background-color:pink;color:black;'><b>LLM Tool Call</b><br><hr><br>get_thermostat_temperature({})</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:32:25 DEBUG:Calling no-arg function: get_thermostat_temperature\n",
      "09:32:25 DEBUG:get_thermostat_temperature called. Returning hardcoded 60\n",
      "09:32:25 DEBUG:‚úîÔ∏è function returned 60\n",
      "09:32:25 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are a helpful assistant that uses the supplied tools to response to the user's questions.\"}, {'role': 'user', 'content': 'Increase the temperature by 10 degrees'}, {'content': None, 'refusal': None, 'role': 'assistant', 'tool_calls': [{'id': 'call_NRZFjLO5krfnRJ5l5sOGZh6c', 'function': {'arguments': '{}', 'name': 'get_thermostat_temperature'}, 'type': 'function'}]}, {'role': 'tool', 'tool_call_id': 'call_NRZFjLO5krfnRJ5l5sOGZh6c', 'content': '60'}], 'model': 'gpt-4o-mini', 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'get_thermostat_temperature', 'description': 'Returns the current temperature setting of the thermostat.\"'}}, {'type': 'function', 'function': {'name': 'set_thermostat_temperature', 'description': 'Sets the current temperature for a given location.\"', 'parameters': {'properties': {'temp': {'description': 'The temperature value in Fahrenheit to set the thermostat to', 'title': 'Temp', 'type': 'number'}}, 'required': ['temp'], 'title': 'SetTemperature', 'type': 'object', 'additionalProperties': False}, 'strict': True}}]}}\n",
      "09:32:25 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "09:32:25 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "09:32:25 DEBUG:send_request_headers.complete\n",
      "09:32:25 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "09:32:25 DEBUG:send_request_body.complete\n",
      "09:32:25 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "09:32:25 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 07 Mar 2025 05:32:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'503'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'199946'), (b'x-ratelimit-reset-requests', b'16.747s'), (b'x-ratelimit-reset-tokens', b'16ms'), (b'x-request-id', b'req_202897554adc3a8865d84a2ddecf276a'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91c7b65f0bbe2a9b-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "09:32:25 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "09:32:25 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "09:32:25 DEBUG:receive_response_body.complete\n",
      "09:32:25 DEBUG:response_closed.started\n",
      "09:32:25 DEBUG:response_closed.complete\n",
      "09:32:25 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 07 Mar 2025 05:32:27 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '503', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '199946', 'x-ratelimit-reset-requests': '16.747s', 'x-ratelimit-reset-tokens': '16ms', 'x-request-id': 'req_202897554adc3a8865d84a2ddecf276a', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '91c7b65f0bbe2a9b-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "09:32:25 DEBUG:request_id: req_202897554adc3a8865d84a2ddecf276a\n",
      "09:32:25 DEBUG:Executing tool_call: ChatCompletionMessageToolCall(id='call_NB3AoGlxZNryvE3EXL10GOoZ', function=Function(arguments='{\"temp\":70}', name='set_thermostat_temperature'), type='function')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;background-color:pink;color:black;'><b>LLM Tool Call</b><br><hr><br>set_thermostat_temperature({\"temp\":70})</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:32:25 DEBUG:Attempting to deserialize {\"temp\":70} for tool: set_thermostat_temperature\n",
      "09:32:25 DEBUG:‚úîÔ∏è deserialized to temp=70.0. Calling function\n",
      "09:32:25 DEBUG:set_thermostat_temperature called with temp=70.0\n",
      "09:32:25 DEBUG:‚úîÔ∏è function returned \n",
      "09:32:25 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are a helpful assistant that uses the supplied tools to response to the user's questions.\"}, {'role': 'user', 'content': 'Increase the temperature by 10 degrees'}, {'content': None, 'refusal': None, 'role': 'assistant', 'tool_calls': [{'id': 'call_NRZFjLO5krfnRJ5l5sOGZh6c', 'function': {'arguments': '{}', 'name': 'get_thermostat_temperature'}, 'type': 'function'}]}, {'role': 'tool', 'tool_call_id': 'call_NRZFjLO5krfnRJ5l5sOGZh6c', 'content': '60'}, {'content': None, 'refusal': None, 'role': 'assistant', 'tool_calls': [{'id': 'call_NB3AoGlxZNryvE3EXL10GOoZ', 'function': {'arguments': '{\"temp\":70}', 'name': 'set_thermostat_temperature'}, 'type': 'function'}]}, {'role': 'tool', 'tool_call_id': 'call_NB3AoGlxZNryvE3EXL10GOoZ', 'content': ''}], 'model': 'gpt-4o-mini', 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'get_thermostat_temperature', 'description': 'Returns the current temperature setting of the thermostat.\"'}}, {'type': 'function', 'function': {'name': 'set_thermostat_temperature', 'description': 'Sets the current temperature for a given location.\"', 'parameters': {'properties': {'temp': {'description': 'The temperature value in Fahrenheit to set the thermostat to', 'title': 'Temp', 'type': 'number'}}, 'required': ['temp'], 'title': 'SetTemperature', 'type': 'object', 'additionalProperties': False}, 'strict': True}}]}}\n",
      "09:32:25 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "09:32:25 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "09:32:25 DEBUG:send_request_headers.complete\n",
      "09:32:25 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "09:32:25 DEBUG:send_request_body.complete\n",
      "09:32:25 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "09:32:26 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 07 Mar 2025 05:32:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'396'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9997'), (b'x-ratelimit-remaining-tokens', b'199943'), (b'x-ratelimit-reset-requests', b'24.738s'), (b'x-ratelimit-reset-tokens', b'16ms'), (b'x-request-id', b'req_119501d3fe084a84a6200b1e200187f6'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91c7b663393b2a9b-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "09:32:26 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "09:32:26 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "09:32:26 DEBUG:receive_response_body.complete\n",
      "09:32:26 DEBUG:response_closed.started\n",
      "09:32:26 DEBUG:response_closed.complete\n",
      "09:32:26 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 07 Mar 2025 05:32:28 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '396', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9997', 'x-ratelimit-remaining-tokens': '199943', 'x-ratelimit-reset-requests': '24.738s', 'x-ratelimit-reset-tokens': '16ms', 'x-request-id': 'req_119501d3fe084a84a6200b1e200187f6', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '91c7b663393b2a9b-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "09:32:26 DEBUG:request_id: req_119501d3fe084a84a6200b1e200187f6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;background-color:pink;color:black;'><b>Final LLM Response</b><br><hr><br>The temperature has been increased by 10 degrees to 70¬∞F.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_chat_loop(\n",
    "    prompt=\"Increase the temperature by 10 degrees\",\n",
    "    tools = [\n",
    "        getToolJsonSchema(get_thermostat_temperature),\n",
    "        getToolJsonSchema(set_thermostat_temperature)\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
