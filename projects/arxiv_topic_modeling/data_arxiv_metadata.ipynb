{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI research Papers\n",
    "\n",
    "> Turns out, the summary information is also provided by the arxiv RSS feed. However, that feed has a limited set of entries. This API allows for a much larger collection of data.\n",
    "\n",
    "Since I am asked to look at AI feeds, I could scan those for links to papers etc. However, the main point was to get papers and then process them. Defaulting to arxiv for now.\n",
    "\n",
    "Normally folks would keep doing this on a weekly or daily basis to keep track of trens _(which are invariably historical comparisons)_. Howver, I have to download a bunch of these to build up my history.\n",
    "\n",
    "Arxiv TOS specifies a rate-limit of 1req/3s which applies to all the machines being controlled by an entity. In my case, just one machine so will see how long it'll take. Maybe cycle through each day/month so I'll atleast have some data for each month and it'll keep adding to it.\n",
    "\n",
    " - python arxiv package\n",
    " - python ratelimit package\n",
    " - PDF to text conversion. Save as json.\n",
    "\n",
    "The result has the following useful attribs (fields/methods) for my use case\n",
    "\n",
    " - 'authors'\n",
    " - 'links'\n",
    " - ✔️'categories'\n",
    " - 'comment'\n",
    " - 'doi'\n",
    " - 'download_pdf' _method_\n",
    " - 'download_source' _method_\n",
    " - 'entry_id'\n",
    " - 'get_short_id' \n",
    " - 'links', \n",
    " - 'pdf_url', \n",
    " - 'primary_category'\n",
    " - ✔️'published'\n",
    " - ✔️'summary'\n",
    " - ✔️'title'\n",
    "\n",
    " If there is a summary, then I don't need to deal with the PDFs, conveting them to text etc yet. Can do at the end if end-to-end gets done!\n",
    "\n",
    "# What to download\n",
    "\n",
    "There are tons of papers every day at arxiv. To manage the load but still get enough time-spread, sample daily.\n",
    " - 10 papers per day in `cs.AI`\n",
    " - Spread over 4 years\n",
    "\n",
    "## Notebook setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths to our libs\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "lib_path = (Path(os.getcwd()) / \"lib\").resolve()\n",
    "sys.path.append(str(lib_path))\n",
    "\n",
    "# Import jupyter utils\n",
    "import logging\n",
    "from util import jupyter_util\n",
    "from util.jupyter_util import DisplayHTML as jh\n",
    "from util.jupyter_util import DisplayMarkdown as jm\n",
    "\n",
    "# Init jupyter env. Set to DEBUG if you want to see the gory details\n",
    "# of schemas and such.\n",
    "jupyter_util.setup_logging(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import logging\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "ARXIV_STRFTIME_FMT = \"%Y%m%d%H%M\"\n",
    "\n",
    "client = arxiv.Client()\n",
    "\n",
    "# allow $calls within $period seconds.\n",
    "# https://info.arxiv.org/help/api/tou.html says no more than 1 call every 3 seconds\n",
    "@sleep_and_retry\n",
    "@limits(calls=1,period=3)\n",
    "def query_arxiv_for_day(start: datetime, max_results: int):\n",
    "    # Day interval\n",
    "    arxiv_from = start.strftime(ARXIV_STRFTIME_FMT)\n",
    "    arxiv_to   = (start + timedelta(hours=23, minutes=59)).strftime(ARXIV_STRFTIME_FMT)\n",
    "\n",
    "    search = arxiv.Search(        \n",
    "        query=f\"cat:cs.AI AND submittedDate:[{str(arxiv_from)} TO {str(arxiv_to)}]\",\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "\n",
    "    logging.debug(f\"Got results for {start.strftime(\"%m/%d/%Y\")}\")\n",
    "    return list(client.results(search))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Unused currently. Will simply load the JSON that is provided by feedreader.\n",
    "class ArxivResultItem(BaseModel):\n",
    "    authors: List[str]\n",
    "    title : str\n",
    "    summary: str\n",
    "    published: str\n",
    "\n",
    "    primary_category: str\n",
    "    categories: List[str]\n",
    "\n",
    "    pdf_url : str\n",
    "    entry_url : str        \n",
    "    \n",
    "    summary: str    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "client = arxiv.Client()\n",
    "\n",
    "def get_arxiv_items(days_lookback:int = 1):        \n",
    "    # Start with today but reset time to start of the day.\n",
    "    today_0 = datetime.today().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    start_t = today_0 - timedelta(days = days_lookback)\n",
    "\n",
    "    # Key this by date\n",
    "    daily_results_dict = {}\n",
    "\n",
    "    for t in [start_t + timedelta(days=d) for d in range(0,days_lookback)]: \n",
    "        day_results = query_arxiv_for_day(t, max_results=10)\n",
    "\n",
    "        day_json_items = []\n",
    "        for r in day_results:\n",
    "            item = ArxivResultItem(\n",
    "                title  = r.title,\n",
    "                authors = [a.name for a in r.authors],\n",
    "                summary = r.summary,\n",
    "\n",
    "                categories = r.categories,\n",
    "                primary_category = r.primary_category,\n",
    "\n",
    "                published = r.published.strftime(\"%m/%d/%Y\"), # good enough\n",
    "                entry_url = r.entry_id,\n",
    "                pdf_url = r.pdf_url\n",
    "            )\n",
    "            day_json_items.append(item)\n",
    "                \n",
    "        daily_results_dict[t] = day_json_items\n",
    "    \n",
    "    return daily_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# while testing use just a day or two.\n",
    "daily_results_dict = get_arxiv_items(\n",
    "    #days_lookback=1\n",
    "    days_lookback=3*365 # 4 years\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine by printing it nicely formatted in markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_arxiv_entries(results_dict_by_day:dict):\n",
    "    print(len(results_dict_by_day))\n",
    "\n",
    "    for day, daily_items in results_dict_by_day.items():\n",
    "        jm.h(f\"Entries for {day.strftime(\"%m/%d/%Y\")}\", level=2)\n",
    "        for e in daily_items:\n",
    "            jm.md(f\"\"\"\n",
    "**Title**: {e.title}\n",
    "\n",
    "**Summary**: {e.summary}\n",
    "\n",
    "----\n",
    "\"\"\")    \n",
    "            \n",
    "#pretty_print_arxiv_entries(daily_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to a single JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(os.getcwd()) / \"data\"\n",
    "FEED_RAW_DATA_DIR = DATA_DIR / \"feed\" / \"raw\"\n",
    "\n",
    "os.makedirs(FEED_RAW_DATA_DIR, exist_ok=True)\n",
    "\n",
    "def results_dict_to_json(results_dict):\n",
    "    json_dict = {}\n",
    "\n",
    "    for day, daily_items in results_dict.items():\n",
    "        day_str = day.strftime(\"%m/%d/%Y\")        \n",
    "        json_dict[day_str] = [result.model_dump() for result in daily_items]\n",
    "    \n",
    "    return json_dict\n",
    "\n",
    "# Write into the raw feeds dir but separate name from the arxiv rss feed.\n",
    "out_path = FEED_RAW_DATA_DIR / \"Arxiv_csAI_API_dailysampled_3y.json\"\n",
    "with open(str(out_path), 'w') as outfile:\n",
    "    json_dict = results_dict_to_json(daily_results_dict)\n",
    "    json.dump(json_dict, outfile, indent=4)\n",
    "\n",
    "logging.debug(f\"Wrote arxiv API dict out to {str(out_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install arxiv\n",
    "import arxiv\n",
    "\n",
    "# https://info.arxiv.org/help/api/user-manual.html#query_details\n",
    "# cat: one of the categories\n",
    "# submittedDate:[202501010000+TO+202501312359] - URL form, note the spaces replaced with +\n",
    "# [YYYYMMDDTTTT+TO+YYYYMMDDTTTT]\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    #query=\"cat:cs.AI AND \",\n",
    "    query=\"cat:cs.AI AND submittedDate:[202501010000 TO 202501302359]\",\n",
    "    max_results = 1,\n",
    "    sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "all_results = list(client.results(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Title : Every Image Listens, Every Image Dances: Music-Driven Image Animation\n",
      "Author : [arxiv.Result.Author('Zhikang Dong'), arxiv.Result.Author('Weituo Hao'), arxiv.Result.Author('Ju-Chiang Wang'), arxiv.Result.Author('Peng Zhang'), arxiv.Result.Author('Pawel Polak')]\n",
      "Link   : [arxiv.Result.Link('http://arxiv.org/abs/2501.18801v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.18801v1', title='pdf', rel='related', content_type=None)]\n",
      "Categories: ['cs.CV', 'cs.AI']\n",
      "entry_id: http://arxiv.org/abs/2501.18801v1\n",
      "pdf_url:  http://arxiv.org/pdf/2501.18801v1\n",
      "primary_category:  cs.CV\n",
      "published:  2025-01-30 23:38:51+00:00\n",
      "summary:  Image animation has become a promising area in multimodal research, with a\n",
      "focus on generating videos from reference images. While prior work has largely\n",
      "emphasized generic video generation guided by text, music-driven dance video\n",
      "generation remains underexplored. In this paper, we introduce MuseDance, an\n",
      "innovative end-to-end model that animates reference images using both music and\n",
      "text inputs. This dual input enables MuseDance to generate personalized videos\n",
      "that follow text descriptions and synchronize character movements with the\n",
      "music. Unlike existing approaches, MuseDance eliminates the need for complex\n",
      "motion guidance inputs, such as pose or depth sequences, making flexible and\n",
      "creative video generation accessible to users of all expertise levels. To\n",
      "advance research in this field, we present a new multimodal dataset comprising\n",
      "2,904 dance videos with corresponding background music and text descriptions.\n",
      "Our approach leverages diffusion-based methods to achieve robust\n",
      "generalization, precise control, and temporal consistency, setting a new\n",
      "baseline for the music-driven image animation task.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I have seen some 100 entries daily. Such productivity!!\n",
    "# - Sample at 5 per day for the last two years\n",
    "\n",
    "# The data in the search is like a digest. The actual PDfs have to be downloaded separately\n",
    "# Still. it takes 40s to get and do a list(results)\n",
    "# 3/10 to 3/18 there are 1000! entries and this took 38 seconds to pull!!\n",
    "# extrapolates to roughly 4000 entries per month at this rate. \n",
    "# \n",
    "# Can I specify search range in the URL ?\n",
    "if len(all_results):\n",
    "    for r in all_results:\n",
    "        print(f\"\"\"\n",
    " Title : {r.title}\n",
    "Author : {r.authors}\n",
    "Link   : {r.links}\n",
    "Categories: {r.categories}\n",
    "entry_id: {r.entry_id}\n",
    "pdf_url:  {r.pdf_url}\n",
    "primary_category:  {r.primary_category}\n",
    "published:  {r.published}\n",
    "summary:  {r.summary}\n",
    "\"\"\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-19 14:46:25.776500\n",
      "2022-03-20 14:46:25.776500\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "t = datetime.today()\n",
    "t_minus_2y = t - timedelta(days=3*365)\n",
    "\n",
    "print(t)\n",
    "print(t_minus_2y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Older ratelimiter is broken in 3.11 as it's use of asyncio.coroutine \n",
    "# has been removed\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "counter = 1\n",
    "\n",
    "# allow $calls within $period seconds.\n",
    "@sleep_and_retry\n",
    "@limits(calls=1,period=1)\n",
    "def do_foo():\n",
    "    global counter\n",
    "    print(counter)\n",
    "    counter += 1\n",
    "\n",
    "for i in range(1,10):\n",
    "    do_foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF for 2022-03-20 00:00:00\n",
      "http://arxiv.org/pdf/2203.10675v1\n",
      "Saving to 2203.10675v1.pdf\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tail_pat = re.compile(r'^.*?/([^/]*)$')\n",
    "\n",
    "for k,v in daily_results_dict.items():\n",
    "    print(f\"PDF for {k}\")\n",
    "\n",
    "    item = v[0]\n",
    "    print(item.pdf_url)\n",
    "    m = tail_pat.match(item.pdf_url)\n",
    "    if m:        \n",
    "        pdf_outfile = f\"{m.group(1)}.pdf\"\n",
    "        print(f\"Saving to {pdf_outfile}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
