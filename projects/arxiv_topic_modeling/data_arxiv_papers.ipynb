{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI research Papers\n",
    "\n",
    "> Turns out, the summary information is also provided by the arxiv RSS feed. However, that feed has a limited set of entries. This API allows for a much larger collection of data.\n",
    "\n",
    "Since I am asked to look at AI feeds, I could scan those for links to papers etc. However, the main point was to get papers and then process them. Defaulting to arxiv for now.\n",
    "\n",
    "Normally folks would keep doing this on a weekly or daily basis to keep track of trens _(which are invariably historical comparisons)_. Howver, I have to download a bunch of these to build up my history.\n",
    "\n",
    "Arxiv TOS specifies a rate-limit of 1req/3s which applies to all the machines being controlled by an entity. In my case, just one machine so will see how long it'll take. Maybe cycle through each day/month so I'll atleast have some data for each month and it'll keep adding to it.\n",
    "\n",
    " - python arxiv package\n",
    " - python ratelimit package\n",
    " - PDF to text conversion. Save as json.\n",
    "\n",
    "The result has the following useful attribs (fields/methods) for my use case\n",
    "\n",
    " - 'authors'\n",
    " - 'links'\n",
    " - ✔️'categories'\n",
    " - 'comment'\n",
    " - 'doi'\n",
    " - 'download_pdf' _method_\n",
    " - 'download_source' _method_\n",
    " - 'entry_id'\n",
    " - 'get_short_id' \n",
    " - 'links', \n",
    " - 'pdf_url', \n",
    " - 'primary_category'\n",
    " - ✔️'published'\n",
    " - ✔️'summary'\n",
    " - ✔️'title'\n",
    "\n",
    " If there is a summary, then I don't need to deal with the PDFs, conveting them to text etc yet. Can do at the end if end-to-end gets done!\n",
    "\n",
    "# What to download\n",
    "\n",
    "There are tons of papers every day at arxiv. To manage the load but still get enough time-spread, sample daily.\n",
    " - 10 papers per day in `cs.AI`\n",
    " - Spread over 4 years\n",
    "\n",
    "## Notebook setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths to our libs\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "lib_path = (Path(os.getcwd()) / \"lib\").resolve()\n",
    "sys.path.append(str(lib_path))\n",
    "\n",
    "# Import jupyter utils\n",
    "import logging\n",
    "from util import jupyter_util\n",
    "from util.jupyter_util import DisplayHTML as jh\n",
    "from util.jupyter_util import DisplayMarkdown as jm\n",
    "\n",
    "# Init jupyter env. Set to DEBUG if you want to see the gory details\n",
    "# of schemas and such.\n",
    "jupyter_util.setup_logging(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move into arxiv_util?\n",
    "from datetime import datetime\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Unused currently. Will simply load the JSON that is provided by feedreader.\n",
    "class ArxivResultItem(BaseModel):\n",
    "    authors: List[str]\n",
    "    title : str\n",
    "    summary: str\n",
    "    published: str\n",
    "\n",
    "    primary_category: str\n",
    "    categories: List[str]\n",
    "\n",
    "    pdf_url : str\n",
    "    entry_url : str        \n",
    "    \n",
    "    summary: str    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:30:04 DEBUG:Finished loading JSON from /home/vamsi/bitbucket/hillops/nbs/BSL_TakeHome/data/feed/raw/Arxiv_csAI_API_dailysampled_3y.json.\n",
      "Have 1095 days worth of records\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "DATA_DIR = Path(os.getcwd()) / \"data\"\n",
    "FEED_RAW_DATA_DIR = DATA_DIR / \"feed\" / \"raw\"\n",
    "\n",
    "# Test with the small 3 day one first\n",
    "#metadata_path = FEED_RAW_DATA_DIR / \"Arxiv_csAI_API_dailysampled_3d.json\"\n",
    "metadata_path = FEED_RAW_DATA_DIR / \"Arxiv_csAI_API_dailysampled_3y.json\"\n",
    "\n",
    "#----------------------------\n",
    "# Read the JSON in\n",
    "arxiv_per_day_mtd = {}\n",
    "with open( str(metadata_path), 'r') as json_data:\n",
    "    arxiv_per_day_mtd = json.load(json_data)\n",
    "\n",
    "logging.debug(f\"Finished loading JSON from {str(metadata_path)}.\\nHave {len(arxiv_per_day_mtd)} days worth of records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=10,period=5)\n",
    "def download_url(url, dest_dir):\n",
    "    fname = url.split(\"/\")[-1]\n",
    "    if not fname.endswith(\".pdf\"):\n",
    "        fname += \".pdf\"\n",
    "\n",
    "    dest_path = Path(dest_dir) / fname\n",
    "    if dest_path.exists():\n",
    "        return        \n",
    "\n",
    "    retry = 3\n",
    "    for r in range(retry):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, str(dest_path))\n",
    "        except Exception as e:\n",
    "            if r < 2:\n",
    "                print(f'Failed. Attempt # {r + 1}')\n",
    "            else:\n",
    "                print(f\"Error encountered at third attempt. Aborting {url}\")\n",
    "                print(e)\n",
    "        else:\n",
    "            #print(f\"Success downloading {url}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare download_list for download\n",
    "import re\n",
    "from dateutil import parser\n",
    "\n",
    "PAPER_DATA_DIR = DATA_DIR / \"arxiv\" / \"pdf\"\n",
    "download_list = []\n",
    "\n",
    "# I have 10 entries per day\n",
    "# To reduce data-load.\n",
    "# Download 1 per day in sequence. 40-70min for each set of 1095 pdfs.\n",
    "# Done: 0, 1, 2, 3, 4\n",
    "nth = 5\n",
    "for k,v in arxiv_per_day_mtd.items():\n",
    "    date = parser.parse(k)\n",
    "    date_folder = date.strftime(\"%m_%d_%Y\")\n",
    "    download_list.append((v[nth][\"pdf_url\"], str(PAPER_DATA_DIR / date_folder)))\n",
    "\n",
    "#print(download_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1095 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n",
      "Failed. Attempt # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 455/1095 [00:15<00:21, 30.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered at third attempt. Aborting http://arxiv.org/pdf/2306.10409v2\n",
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 625/1095 [00:49<01:21,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 670/1095 [03:21<16:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 686/1095 [03:59<22:02,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n",
      "Failed. Attempt # 2\n",
      "Error encountered at third attempt. Aborting http://arxiv.org/pdf/2402.05951v3\n",
      "HTTP Error 404: NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 692/1095 [04:08<08:59,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 702/1095 [04:31<12:33,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 711/1095 [04:57<20:08,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 785/1095 [10:40<04:25,  1.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n",
      "Failed. Attempt # 2\n",
      "Error encountered at third attempt. Aborting http://arxiv.org/pdf/2405.08031v2\n",
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 858/1095 [13:54<08:43,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 868/1095 [14:37<05:50,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 961/1095 [20:45<04:46,  2.14s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 996/1095 [22:41<03:25,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1004/1095 [23:06<02:32,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1032/1095 [24:20<05:20,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1059/1095 [25:41<00:21,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed. Attempt # 1\n",
      "Failed. Attempt # 2\n",
      "Error encountered at third attempt. Aborting http://arxiv.org/pdf/2502.07115v3\n",
      "HTTP Error 404: NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1095/1095 [26:26<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# After first pass, no need to do os.makedirs. Simply takes up time\n",
    "DO_MAKE_DIRS = False\n",
    "\n",
    "for (url, dest_dir) in tqdm(download_list):\n",
    "    fname = url.split(\"/\")[-1]\n",
    "    if not fname.endswith(\".pdf\"):\n",
    "        fname += \".pdf\"\n",
    "\n",
    "    dest_path = Path(dest_dir) / fname\n",
    "    if not dest_path.exists():            \n",
    "        if DO_MAKE_DIRS: \n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            \n",
    "        download_url(url, dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from \n",
    "# https://gist.github.com/darwing1210/c9ff8e3af8ba832e38e6e6e347d9047a\n",
    "# And modified to be per-day\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "import aiohttp  # pip install aiohttp\n",
    "import aiofile  # pip install aiofile\n",
    "\n",
    "def download_files_from_report(urls_folder_list):\n",
    "\n",
    "    # Create all dirs needed\n",
    "    for (_, folder) in urls_folder_list:\n",
    "          os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    sema = asyncio.BoundedSemaphore(5)\n",
    "\n",
    "    async def fetch_file(session, url, out_dir):\n",
    "        fname = url.split(\"/\")[-1]\n",
    "        if not fname.endswith(\".pdf\"):\n",
    "            fname += \".pdf\"\n",
    "            \n",
    "        async with sema:\n",
    "            logging.debug(f\"Queing {url}\")\n",
    "            async with session.get(url) as resp:\n",
    "                assert resp.status == 200\n",
    "                data = await resp.read()\n",
    "\n",
    "        async with aiofile.async_open(\n",
    "            os.path.join(out_dir, fname), \"wb\"\n",
    "        ) as outfile:\n",
    "            await outfile.write(data)\n",
    "\n",
    "    async def main():\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = [fetch_file(session, url, out_dir) for (url, out_dir) in urls_folder_list]\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(main())\n",
    "    #loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success downloading http://arxiv.org/pdf/2503.12688v1\n",
      "Success downloading http://arxiv.org/pdf/2503.12687v1\n",
      "Success downloading http://arxiv.org/pdf/2503.12667v1\n",
      "Success downloading http://arxiv.org/pdf/2503.12651v1\n",
      "Success downloading http://arxiv.org/pdf/2503.12649v1\n",
      "Success downloading http://arxiv.org/pdf/2503.12642v1\n",
      "Success downloading http://arxiv.org/pdf/2503.13554v1\n",
      "Success downloading http://arxiv.org/pdf/2503.13553v1\n",
      "Success downloading http://arxiv.org/pdf/2503.12637v1\n",
      "Success downloading http://arxiv.org/pdf/2503.12635v1\n",
      "Success downloading http://arxiv.org/pdf/2503.13778v1\n",
      "Success downloading http://arxiv.org/pdf/2503.13771v1\n",
      "Success downloading http://arxiv.org/pdf/2503.13754v2\n",
      "Success downloading http://arxiv.org/pdf/2503.13751v1\n",
      "Success downloading http://arxiv.org/pdf/2503.13708v1\n",
      "Success downloading http://arxiv.org/pdf/2503.13690v1\n",
      "Success downloading http://arxiv.org/pdf/2503.13660v1\n",
      "Success downloading http://arxiv.org/pdf/2503.13657v1\n",
      "Success downloading http://arxiv.org/pdf/2503.13621v1\n",
      "Success downloading http://arxiv.org/pdf/2503.13447v1\n",
      "Success downloading http://arxiv.org/pdf/2503.14505v1\n",
      "Success downloading http://arxiv.org/pdf/2503.14503v1\n",
      "Success downloading http://arxiv.org/pdf/2503.14499v1\n",
      "Success downloading http://arxiv.org/pdf/2503.14493v1\n",
      "Success downloading http://arxiv.org/pdf/2503.14492v1\n",
      "Success downloading http://arxiv.org/pdf/2503.14488v1\n",
      "Success downloading http://arxiv.org/pdf/2503.14487v1\n",
      "Success downloading http://arxiv.org/pdf/2503.14484v1\n",
      "Success downloading http://arxiv.org/pdf/2503.14469v1\n",
      "Success downloading http://arxiv.org/pdf/2503.14456v1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "fn = \"hello.pdf\"\n",
    "if fn.endswith(\".pdf\"):\n",
    "    print(\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
