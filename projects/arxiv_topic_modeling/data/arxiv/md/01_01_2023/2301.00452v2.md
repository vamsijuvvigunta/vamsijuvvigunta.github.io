## Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning

### Yonghao Long, Wang Wei, Tao Huang, Yuehao Wang and Qi Dou The Chinese University of Hong Kong


**_Abstract— Surgical robot automation has attracted increas-_**
**ing research interest over the past decade, expecting its potential**
**to benefit surgeons, nurses and patients. Recently, the learning**
**paradigm of embodied intelligence has demonstrated promising**
**ability to learn good control policies for various complex tasks,**
**where embodied AI simulators play an essential role to facilitate**
**relevant research. However, existing open-sourced simulators**
**for surgical robot are still not sufficiently supporting human**
**interactions through physical input devices, which further**
**limits effective investigations on how the human demonstrations**
**would affect policy learning. In this work, we study human-**
**in-the-loop** **embodied** **intelligence** **with** **a** **new** **interactive**
**simulation platform for surgical robot learning. Specifically, we**
**establish our platform based on our previously released SurRoL**
**simulator with several new features co-developed to allow high-**
**quality human interaction via an input device. We showcase the**
**improvement of our simulation environment with the designed**
**new features, and validate effectiveness of incorporating human**
**factors in embodied intelligence through the use of human**
**demonstrations and reinforcement learning as a representative**
**example. Promising results are obtained in terms of learning**
**efficiency. Lastly, five new surgical robot training tasks are**
**developed and released, with which we hope to pave the way**
**for future research on surgical embodied intelligence. Our**
**learning platform is publicly released and will be continuously**
**[updated in the website: https://med-air.github.io/SurRoL.](https://med-air.github.io/SurRoL)**

I. INTRODUCTION


Demos Buffer

|Observation|SurRoL|………|
|---|---|---|


**Surgical Robot Simulator**

Observation


Observation

Human Action SurRoL

………


SurRoL
………


**Surgical Robot Simulator**


SurRoL


**Policy Learning**


**Surgical Robot Simulator**


Observation

Human Action SurRoL

………


Surgical robotics has developed rapidly in the past decade
and transformed minimally invasive surgery in practice [1].
Recently, surgical task automation [2] has received increasing
attention from researchers as it is promising to reduce burden
of surgeons and improve operational efficiency [3]. However,
it still remains a distant dream with challenges from complex
surgical scenes and skillful surgical actions. Conventional
solutions typically rely on heuristic planning methods while
struggle to yield scalable control policies. To date, successful
stories on surgical task automation are still in its infancy [4].
Embodied intelligence [5] hypothesizes to directly learn
various skills based on interactions between robots and the
environment, which has demonstrated remarkable capability
on robot task automation [6]. In this context, simulators [7–
9] serve as the essential infrastructure to provide a digital

This project was supported in part by Hong Kong Research Grants
Council TRS Project No.T42-409/18-R, in part by Hong Kong Innovation
and Technology Commission under Project No. PRP/026/22FX, in part by
Multi-Scale Medical Robotics Center InnoHK under grant 8312051, and
in part by a Research Fund from Cornerstone Robotics Ltd.
Y. Long, W. Wei, T. Huang, Y. Wang, and Q. Dou are with the
Department of Computer Science and Engineering, The Chinese University
of Hong Kong. Q. Dou is also with the T Stone Robotics Institute, CUHK.
_Corresponding author: Qi Dou (qidou@cuhk.edu.hk)._


Observation

Human Action SurRoL


Fig. 1. Illustration for the concept of surgical embodied intelligence with
human-in-the-loop demonstrations for robot learning.

twin [10, 11] of the physical world, in order to facilitate collection of interactive data, training and testing of the agent.
Reinforcement learning (RL) [12] is typically used together
with embodied AI, which can model the task execution as
Markov Decision Process and optimize the policy learning
through robot-environment interactions in the simulator. Despite visible efforts that have been made on embodied intelligence in general [7, 8, 13], surgical embodied intelligence,
which should be supported by tailored and domain-specific
simulation environments still remains to be further explored.
Specifically, early works [14, 15] focusing on simulation
of surgical robot and its relevant tasks (such as peg transfer)
were concentrated on realistic virtual environment creation,
rather than serving for AI purposes as is desired nowadays.
Recent simulators increasingly aim at bridging embodied
intelligence, in particular reinforcement learning, to surgical
robots for developing advanced methods for task automation.
Richter et al. proposed dVRL [16], the first open-sourced
reinforcement learning environment for surgical robotics,
and showed that the learned policies can be deployed to the
dVRK platform. However, it has not included an interface
for physical input device, therefore could not support
manual inputs through intuitive interactions. This further
restricts the functionality on collecting human demonstration
data to study machine learning algorithms. Tagliabue et al.
developed UnityFlexML [17] for effective RL policy learning
on tasks involving soft-tissue manipulations. It includes a
small number of sheet-like virtual assets to date, which
may limit the diversity of surgical tasks or scenarios that
can be investigated. Very recently, Munawar et al. released
a comprehensive simulation platform AMBF [18] which
supports interactive human inputs, therefore it can collect
data to train and test control mechanisms for surgical robot.
However, AMBF is not yet sufficiently supporting AI algorithmic libraries to facilitate people to explore reinforcement


-----

learning methods using large-scale human demonstrations.
In parallel with the current defect in simulators, a key
unsolved technical pursuit of surgical robot learning is how
to pursue a higher level of autonomy [19, 20]. The solution to
this problem needs to note that most existing surgical robots
still use a human-centered manner (i.e. teleoperation) [2, 21].
In other words, AI developments should incorporate human
factors into the loop in order to make use of expert’s
knowledge for promoting cognitive ability to learn complex
surgical skills [22]. However, achieving human-in-the-loop
surgical embodied intelligence involves many challenges.
First is how to establish an accurate and intuitive action mapping mechanism between human input device and surgical
robot, which should be standardized and compatible for machine learning. Second is how to build realistic physical simulation accompanied with high-fidelity scene visualisation to
provide immersive feedback from the virtual environment
upon human interactions. Third is how to understand the
effect of human demonstrations on policy learning for task
automation in the context of surgical embodied AI.
To address above challenges, we propose to investigate
human-in-the-loop embodied intelligence with an interactive
simulator dedicated for surgical robot learning, with the
concept illustrated in Fig. 1. A human (usually an expert
surgeon) can manipulate virtual objects via virtual robot
arms through interaction with a physical input device, and
in turn perceive visual feedback on such interactions in
the simulator. The human demonstrations can be saved
to a database in the form of pairs of end-effector actions
(variation of position, orientation and gripper angle) and
environmental states, which are then used for control policy
learning with RL. Besides, the robot can also explore
the environment by itself through trial-and-errors (i.e.
action-state pairs) in a typical embodied AI paradigm.
In this work, to achieve the goal, we develop the platform
based on our existing open-source simulator SurRoL [23],
i.e., a RL-centered and dVRK compatible surgical robot
simulator. The proposed new version highlights the
introduction of human interaction which is achieved by an
interface of haptic input devices for two hands. Importantly, a
set of new features are co-developed to establish the realistic
human interaction: 1) standardized interface which supports
human interaction through physical input device and policy
learning, 2) realistic physical simulation with fine-grained
modeling of collision properties and proportional derivative
(PD) control, 3) high-fidelity rendering with vivid phong
shading, spotlight modeling and shadow projection for
human perception. On top of these, we further use collected
human demonstration data to train policies for surgical robot
task automation, and compare its performance with previous
code-generated demonstration data. Promising results are
achieved in terms of efficiency for control policy learning.
Finally, five new surgical training tasks (i.e., PickAndPlace,
_PegBoard, NeedleRings, MatchBoard, MatchBoardPanel)_
are added into the simulator which can support surgical skill
training of practitioners. With these tasks, we aim to facilitate
the future investigation of surgical robot learning on complex


training tasks, and human-in-the-loop surgical embodied
AI. Our code is available as an updated branch within the
[SurRoL repository at: https://github.com/med-air/SurRoL.](https://github.com/med-air/SurRoL)

II. RELATED WORK

_A. Embodied AI for Robots with Interactive Simulators_

Embodied AI aims to learn complex skills through
interaction with the environment, instead of directly
learning from pre-collected datasets counting video, audio,
image and text. Its rapid progress requires and promotes
the development of simulators for embodied AI [5].
Several simulators with noticeable contributions from
embodied AI community all incorporate interaction as
an important feature. Xiang et al. proposed SAPIEN [9],
which enables robotic interaction tasks with a realistic and
physics-rich simulated environment. Kiana et al. proposed
ManipulaTHOR [24] to facilitate research on visual object
manipulation using a robotic arm. Recently, embodied
AI with human interaction is gaining more and more
attention. Li et al. proposed iGibson [8] to simulate multiple
household tasks which allows VR-based human-environment
interaction. Gan et al. proposed ThreeDworld [25] for
interactive multi-modal physical simulation which supports
human interaction through VR devices. Fu et al. proposed
RFUniverse [26], a physics-based action-centric environment
for learning household tasks, which also provides VR
interface for human input. Some studies [7, 27] show that
with human data involved, the intelligent algorithms tend
to demonstrate better performance. Still, there are few
dedicated attempts on simulators in surgical robot learning
research topic, due to major challenges on developing opensource software infrastructures which support high-quality
human and multiple surgical tasks for study.

_B. Learning-based Surgical Task Automation_

Learning-based surgical task automation is an emerging
field in surgical robotics that aims to automate surgical
tasks using machine learning techniques. Various surgical
tasks have been studied in this field, including suturing [28],
pattern cutting [29], tissue manipulation [30], etc. One of the
commonly used approaches is imitation learning [31], where
a policy is trained on a dataset of expert demonstrations
through supervised learning technique. However, this method
cannot handle distribution shift thus suffering from poor generalizability. On the other hand, RL [32, 33], which allows
the robot to learn by interacting with the environment, shows
outstanding capability of generalizing to learn different tasks.
However, traditional RL will suffer from a large exploration
burden, which is time-consuming and resource-intensive
with no guarantee of success. Recently, there has been a
growing interest in leveraging demonstration to improve the
learning efficiency of RL [34–36], which achieved promising
results. This type of methods are commonly studied with an
interactive simulation environment for learning and testing
due to the potential risks associated with real surgical
procedures and the difficulty in obtaining comprehensive
and accurate [37] surgical data. However, much developing


-----

**Visualization Engine**

Vivid Phong Shading

Spot Light Modeling

Shadow Projection


**Surgical Tasks**

Assets Creation from Blender

Efficient Collision Modelling

Position/Orientation Mapping


**Env State**

Observation

Reward

Action


**OBS**


SWIG Python API


Fig. 2. The proposed platform of human-in-the-loop surgical embodied intelligence with interactive simulation environment for surgical robot learning.


**Physical Simulation**

Articulated Bodies URDF

Robot Kinematics Control

Collision Detection/Query


**Neural Network**


Mapping

InstrumentYaw Angle Pitch AngleInstrument

Effector PositionInstrument End

Simulator WorldCoordinate


SWIG Python API


workload is always accompanied to establish a simulator.
In the light of this, we are dedicated to developing a opensourced surgical robot simulator supporting high-quality
human interaction and multiple tasks. With which we hope
to accelerate progress in this rapidly evolving field.

III. MATERIALS AND METHODS

_A. Simulation Platform Development as Infrastructure_

Our overall system consists of three main components:
1) human interaction interface, 2) intelligent policy learning, and 3) surgical embodied AI virtual environment. The
overview framework is illustrated in Fig. 2. First, we create
assets of surgical skill training tasks with the help of the 3D
modeling tool Blender, and then generate relevant collision
models and URDF description models at the same time for
physical modeling in the simulation environment. Once surgical tasks are imported to the simulator, the human can conduct surgical action via a manual interaction device, and the
interaction information is streamed to the virtual environment
for physical simulation. In the meanwhile, the video frames
are produced using the visualization engine of the simulator,
which will be displayed on the monitor for human perception
and next-step interaction. The policy can learn through
interaction with the virtual environment by itself and also
learn from human through recorded expert demonstration.

_B. Kinematics Mapping and Control with Input Device_

To incorporate human control of surgical robots in the simulator, the first and essential step is to develop a manipulation
system with physical input devices. In this paper, we opt for
_Touch [38] (3D Systems Inc.) as the typical input devices_
of the simulator owing to its advantages of high stability,
customizability, wide adoption [39, 40] and low cost. Specifically, two Touch devices are used to simulate the two master
arms of the robot to teleoperate the Patient Side Manipulators
(PSMs) and the Endoscopic Camera Manipulator (ECM).


**OpenHaptic Lib**

Device Driver

1 kHZ servo loop

HD API

SWIG Python API


**OBSOUT**


_1) Kinematics mapping from input device to virtual robot:_
To enable smooth control between the physical input device
and surgical robots in the simulator, we map the current
action of the end-effector instead of pose or joint angle
from input device to the simulator, as it is more intuitive
and compatible with the policy action (such as that from
RL policy), which can also facilitate the recording of human
demonstrations. In specific, for each simulation step k, we
first retrieve the end-effector’s position and joint angle of
current step p(k) = {x(k), y(k), z(k), rotate(k)} and its
previous step p(k _−1) from haptic (as shown in Fig. 2, phys-_
ical input device). Then, we calculate the current action as
_p(k) −_ _p(k −_ 1) = {dx, dy, dz, drotate}, where dx, dy, dz determine the position movement in the Cartesian space, drotate
determines the orientation change. For PSM, drotate is yaw
or pitch angle for top-down or vertical space setting, which
can be adapted to meet specific surgical needs. For ECM,
_drotate is the roll angle which allows the surgeon to adjust the_
camera’s angle around its longitudinal axis. When Button 1
(shown in Fig. 2, physical input device) is pressed, the angle
of instrument jaw j(k) decreases a constant value for each
step until it is closed (j(k) < 0), while it is released, the j(k)
increases until it is fully open. When Button 2 is pressed,
all actions are set to null to simulate the clutch mechanism
of the control, which is usually used to adjust the master
workspace during the operation. All movement actions will
be multiplied by scaling vectors (corresponding to the tool
movement scale), which will then be added to the current
state of the surgical instrument to yield the target pose.
_2) Instrumental end-effector control:_ After the action
mapping, we realize the surgical robot control with inverse
kinematic (IK) and a PD controller. Specifically, given the
target pose of the surgical instrument end-effector, the target
joint angles of the surgical robot are calculated using the
IK based on Denavit-Hartenberg (DH) parameters (which
are consistent with dVRK [2]). As the PSMs and ECM are


Haptic (Open/close)Button 1 Button 2 (clutch) Mapping
Pitch Angle

Haptic BaseCoordinate Yaw AngleHaptic

Haptic End
Effector Position


-----

respectively 6 Degree-of-freedom (DOF) and 4 DOF with a
remote center of motion (RCM), the analytical solutions for
both of them exist. Given the target and current pose of the
instrument, we enable the realistic and smooth movement
control of the robot using position and velocity PD control.
The error is designed as:

_e(tk) = K1 · (ptarget(tk) −_ _pcurrent(tk)) + K2 · (vtarget(tk) −_ _vcurrent(tk)),_

(1)
And the discrete control system [41] is formulated as:

_u(tk) =_ �Kp + _[K]∆[d]t_ � _e(tk) −_ �Kp + [2]∆[K]t[d] � _e(tk−1) +_ _[K]∆[d]t_ _[e][(][t][k][−][2][)][,][ (2)]_


where u(tk) represents the system output action, K1 and
_K2 stand for proportional and integral gains, and ∆t is_
the time interval. To maximize the efficiency of highfrequent controlling communication between haptic input
and simulator, we leverage SWIG [42] to directly wrap the
HD API of OpenHaptics (original Haptic SDK which is
implemented using C/C++) into Python to bridge the haptic
device with our python-based environment.

_C. Realistic Physical Interaction Simulation_

Introducing human interactions into the simulator will also
introduce some unexpected actions (e.g., sudden movement
and destructive behavior). In this regard, we need to further
optimize the physical simulation and interaction based on
SurRoL for a realistic manipulation user experience.
Firstly, to enable realistic simulation of different objects
including the surgical instruments, all their articulated bodies
are modeled following the real-world ratio using Blender.
Meanwhile, the contact and inertia attributes (e.g. stiffness
and mass) in objects’ URDF files are first initialized with
the real-world measurements and then adjusted manually
through trail-and-error simulation. The adjustments are repeated iteratively until the simulation results match the realworld as closely as possible. Besides, given that most object
assets in surgical tasks are more complicated than a single
convex hull or primitive (e.g., boxes, cylinders, and spheres)

[18], convex decomposition by V-HACD [43] and collision
primitive compound are applied to the meshes to get collision
geometry in objects’ URDF files for more precise collision
detection. However, when there are some destructive
behaviors from humans, such as pushing or grasping objects
with a very large motion, the inter-penetration problem could
happen among surgical instruments and objects, which hurts
realism. This problem arises when assets collide with each
other with abnormally large speed, causing collision solving
failed and making them intersected or overlapped. Moreover,
the sudden movement of the tool from one place to another
can not be achieved in real surgical robots. Therefore,
designing the controller with constraints on force and
velocity is of vital importance. Specifically, based on position
and velocity PD control mode, we impose constraints on the
output of joint motors when controlling PSMs and ECM.
During the step simulation, the underlying designed control
algorithms will calculate motor actions under the maximum
motor force and velocity limitation to reach the target


Fig. 3. The example results (a) before and (b) after optimizing physical
modeling in PegTransfer task (from SurRoL). It shows our proposed method
can prevent inter-penetration problem under unexpected movement, yielding
more realistic interaction when human manipulating in the simulator.

position. In addition, joint constraints are also utilized in the
interaction between grippers and objects for more stable and
realistic grasping. An example comparison of before and
after optimization of the physical simulation is illustrated
in Fig. 3, which shows our proposed solutions can prevent
inter-penetration problem, yielding more realistic interaction.

_D. High-fidelity Surgical Scene Rendering_

To develop an authentic human interaction in the simulator, providing the rendered scene with high visual realism is
important for for human perception in virtual-reality based
surgical training. However, the physical simulation backend
in original SurRoL (using PyBullet) does not focus on
rendering realism for human interaction thus only supports
simple scene rendering for result visualization purpose. To
address this limitation, we proposed a practical rendering
plug-in for PyBullet engine which supports Panda3D [44] (an
open source framework for 3D rendering with Python API).
Our developed plug-in bridges the gap between PyBullet
and Panda3D to enable realistic rendering of scenes.
Specifically, we rewrote the underlying rendering interface
of PyBullet so that it can support conversion of the properties
of assets, such as pose, texture, and material from PyBullet
data format to Panda3D readable format, and then pass them
to Panda3D for rendering. We adopt the phong shading [45]
model from Panda3D to simulate the lighting reflection and
diffusion of the whole virtual environment. Compared to
the traditional rendering pipeline [46] using texture, normal
maps and specular maps to control the realism of the scene,
the adopted one is more similar to the physics of the real
world. Moreover, the traditional way of simulating lighting
sources is using directional light or ambient models. They
are inconsistent with the endoscope light which adopts fiber
optic cable to guide external light sources to the end of the
endoscopy and emit an intensive cone beam directionally
toward the area of interest (surgical scene). To simulate
this kind of lighting condition, the spotlight model, which
can replicate this effect by focusing the light in a particular
direction with field-of-view, is leveraged to achieve a
realistic simulation of surgical robot endoscope lighting. In
addition, shadow mapping is enabled to simulate the light
occlusion and visualize the projected shadow. Last but not
least, the previous rendering engine will produce images
with an obvious aliasing effect which can affect the human
perceptual experience to a large degree. As a result, we
enable the anti-aliasing in the rendering pipeline for a more
natural visualization. The comparison of the visualization
results is shown in Fig. 4, which demonstrates the proposed


-----

a)

b)


a)

b)

_PickAndPlace_ _PegBoard_ _NeedleRings_ _MatchBoard_ _MatchBoardPanel_


Fig. 4. The visualization results (a) using original rendering engine from SurRoL, and results (b) using optimized engine for realistic rendering. Five
tasks from left to right are PickAndPlace, PegBoard, NeedleRings, MatchBoard and MatchBoardPanel respectively, which are described in Sec. III-E.


method can generate more realistic visualizations in terms of
lighting, shadow, reflection and fidelity. Finally, in the spirit
of user-friendliness, we further leverage Panda3D to develop
an graphical user interface (GUI) (as illustrated in Fig. 2,
Monitor). It allows trainee to conveniently select different
surgical training task in a panel through clicking the “play”
button, and exit the task through “exit” button, which can
be flexibly further extended and customized as needed.
Notably, the proposed pipeline can not only improve the
rendering realism for human interaction, but also can largely
facilitate the further research on image-based perception and
sim-to-real tasks, such as visual reinforcement learning [33],
surgical scene segmentation and action recognition, bridging
the gap between the virtual environment and the real world.

_E. Surgical Tasks for Both Training and Automation_

In the previous version of SurRoL simulator, we designed
several surgical robot tasks (e.g., NeedlePick, NeedleRegrasp
and EcmReach) to specifically evaluate the robot learning
algorithms. Still, these tasks are not sufficient to fulfill the requirements of current and future comprehensive research on
surgical training [47, 48], where human interaction is a very
important factor. To this end, we add five new tasks following
the common curriculum tasks in robotic surgery simulationbased training [49–51], which are representative of the evaluation of both surgical training and task automation. Specifically, we add new tasks of PickAndPlace, PegBoard, Need_leRings, MatchBoard and MatchBoardPanel (see Fig. 4). In_
task PickAndPlace, the trainee needs to pick up the colored
jacks and place them in the tray in the same color. The task
will be considered as success only when all the colored jacks
are placed on the corresponding trays. In task PegBoard, the
trainee needs to pick up the ring from the vertical peg board
and then place it on the peg from the horizontal board, which
requires high proficiency in controlling the pose and orientation of the rings thus effective for trainees to practice their
manipulation skills. In task NeedleRings, the trainee needs to
hand off the needle using two robot arms to pass through the
ring, which calls for trainees to master proficient two-handed
needle manipulation skills in terms of precise position and
orientation control. In task MatchBoard, the trainee needs to
pick up a digit or alphabet block and place it on groove in


the corresponding position, which further practices hand-eye
coordination. In task MatchBoardPanel, the trainee needs to
first grasp the drawer handle to open the drawer and then pick
up a digit or alphabet block to place it in the targeted grid,
which is an advanced skills training tasks with sequential
sub-tasks. These tasks are specifically designed and modeled
which can be easily extended and applied for further research
on surgical training and human-in-the-loop robot learning.

_F. Surgical Skills Learning from Human Demonstrations_

While traditional methods of learning from demonstration
often involve exploiting data and knowledge from machinegenerated demonstrations [52, 53], such approaches may
not be effective for surgical tasks, which require delicate
and skillful surgical operations by surgeons that cannot be
interpreted through straightforward actions. Therefore, in the
case of robotic surgery, the primary focus of learning from
demonstration is on expert human demonstrations [36, 54].
In this regard, we opt for human demonstration-guided RL
as example to validate our proposed interactive simulator.
_1) Formulation: Specifically, we consider an RL agent_
interacts with our simulator formulated as a Markov Decision
Process. The agent takes an action ak ∈A at state sk ∈S
according to its policy π : S →A at each time step k. It
then receives a reward signal rk and transits to the successor
state sk+1, repeating policy execution until the episode ends,
where each experience (sk, ak, rk, sk+1) is stored into a
replay buffer DA. Meanwhile, the agent maintains another
buffer DE that includes expert demonstrations given in
advance. The goal of the agent is to find an optimal policy
_π[⋆]_ that maximizes the expectation of discounted cumulative
reward (a.k.a. return) with a discount factor γ ∈ (0, 1]. Many
RL methods achieve this by estimating Q-value function
_Q : S ×A →_ R that gives the expected return of action ak at
state sk. We herein adopt deep deterministic policy gradient
DDPG [55], a widely-used deep RL method in surgical
robot learning [16, 30], to learn a Q-value function Qθ with
parameters θ by minimizing the squared Bellman error [55]:

_LQ(θ) = EDA_ �(rk + γQθ(sk+1, ak+1) − _Qθ(sk, ak))[2][�]_ _. (3)_

To exploit the knowledge from demonstrations, we
follow the approach in [56] that utilizes state-action pairs


-----

from demonstrations to encourage the behavioral similarity
between agent and expert. It realizes this by first pre-training
the policy πϕ parameterized by ϕ with behavior cloning
loss [57], and then minimizing the following objective at
the online stage:

_Lπ(ϕ) = Es∈DA [−Qθ(sk, πϕ(sk))] + Es,a∈DE_ �∥πϕ(sk) − _ak∥[2]2�_ _._
(4)
We further adopt hindsight experience replay HER [58]
as a sampling strategy for both buffers, which addresses the
sparse reward issue for goal-conditioned environments.
_2) Experiment_ _setup:_ We choose _NeedlePick_ as a
representative task for validation, which is an essential
surgical training task, where the user needs to manipulate
the robot arm to reach and pick up the needle on the tray,
and then move it to a targeted location. It contains multiple
interactions with the virtual environment thus relevant
surgical skill is needed to successfully complete the task. The
reward function is defined following the previous SurRoL as:
_r(s, a) = −I(∥og−oc∥2≥ϵ), where oc and og are respectively_
current and goal location of the needle center, while ϵ is the
tolerant distance between them. During the experiment, we
collect the successful human expert and non-expert demonstrations using the proposed simulator. Meanwhile, we also
developed a linear path planning script which can generate a
sequence of waypoints between the robot’s current position
and the target position to collect script demonstrations. Then
we compare the results of using human expert, non-expert
demonstrations and the result of using script demonstrations.
To evaluate the learned policies, we opt for success rate,
steps to complete (time cost), and economy of motion
(trajectory distance) as the evaluation metrics, which have
been commonly adopted to evaluate the surgeons’ skills in
the robotic surgery simulation training [15, 50, 51], owing to
their effectiveness of representing the level of surgical skills.
_3) Implementation_ _details:_ Following the setting in
SurRoL, the workspace is set to 10.0cm[2], and the tolerance
distance between the goal and current state is 0.5cm. Each
training epoch contains 40 episodes and each episode is set
to 150 timesteps. Simulation time step is set to 0.2 seconds.
The environment will be reset if the user fails within
the defined timesteps. The initial and goal conditions are
randomly sampled every time the environment resets. The
policy is modeled as four-layer MLPs with ReLU activations,
where each layer was of 256 hidden dimensions. We build
our implementation using DEX [59] framework, which is on
top of OpenAI baselines [60] and use their hyper-parameter
settings for fair comparison. For demonstration collection,
we record 100 human expert demonstrations from four
medical students (each 25), who had conducted robotic
surgical training and fluent at using our simulator. In
addition, we record 100 non-expert human demonstrations
from four engineering students (each 25), who had hands-on
experience using our simulator. Meanwhile, we generated
additional 100 script demonstrations through our designed
path planning script for comparison. For fairness, the average
completion time steps of demonstrations are numerically


0.0

0 25 50 75 100
Training Epochs

Fig. 5. The success rate learning curve for NeedlePick with a sliding
window smooth. Results show that policy learned from human expert
demonstrations consistently outperforms the policy learned from non-expert
demonstrations or script demonstrations. The shaded region represents the
standard deviation over four random seeds.


close with around 113 steps for the human expert, 117 for
non-expert and 109 steps for the script. Similarly, the average
trajectory distances are 12.1cm for human expert, 12.5cm
for non-expert and 11.7cm for script demonstrations.

IV. RESULTS


Current scheme of evaluating surgical simulation training
through proficiency-based progression (PBP) [61, 62] emphasizes the importance of learning efficiency to assess the
level of surgical skills, which parallels the concept in previous works on RL [63, 64] that advocate for evaluating the
sample-efficiency of proposed algorithms at the early stage
of training. In the light of this, to analyze the learning process
and policy performance, we analyse the training curve of the
policy and the results at 50 epochs (early training stage) and
100 epochs (when all the results show no increasing trend).
To demonstrate the learning efficiency of the policies, we
train them using four seeds and plot the averaged success
rates with results shown in Fig. 5. We observe that learning
from human expert demonstration consistently outperform
learning from non-expert and script demonstrations in terms
of the success rate over the training process. Specifically,
the learning curves of RL policies from human expert
demonstrations have smaller deviations than non-expert and
script, indicating the improvement in stability of policy
learning. After initial training for 50 epochs, the averaged
success rate of NeedlePick achieves 89.7% ± 6.4% when
learning from the human expert, outperforming learning
from the non-expert (66.3% ± 12.7% success rate) by
23.4%, and outperforming script (43.2% ± 21.9% success
rate) by 46.5% (as shown in Table I). The policy learned
from human expert demonstration can also master a faster
completion time steps (39.2 ± 14.5) compared to non-expert
(59.5 ± 17.1) and script (101.8 ± 27.1). Moreover, we
calculate the average economy of motion and find that
the motion trajectories are shorter with 12.2cm ± 1.3cm
when using human expert demonstration, 14.2cm ± 3.1cm
for non-expert and 15.0cm ± 2.5cm for script. After
training for 100 epochs, the results of learning from human
demonstration are more stable than non-expert and script
with smaller variance. Meanwhile, the policy learned from
human expert demonstration consistently outperformed the

1.0


0.8

0.6


0.4

0.2


-----

TABLE I
**THE EVALUATION RESULTS OF NeedlePick IN SIMULATOR.**

|Types of Demo|50 epochs|100 epochs|
|---|---|---|
||Script Non-expert Human Expert|Script Non-expert Human Expert|
|Success Rate / % (↑) Steps to Complete (↓) Economy of Motion / cm (↓)|43.2 (±21.9) 66.3 (±12.7) 89.7 (±6.4) 101.8 (±27.1) 59.5 (±17.1) 39.2 (±14.5) 15.0 (±2.5) 14.2 (±3.1) 12.2 (±1.3)|93.2 (±6.9) 86.7 (±2.4) 99.3 (±0.1) 24.0 (±10.4) 21.4 (±5.9) 15.0 (±5.5) 13.5 (±1.6) 12.6 (±2.9) 11.6 (±1.1)|


Fig. 6. An example case shows that at early training stage, the policy
learned from human demos can master skills of handling failure attempts
and then complete the task, while learning from script demos cannot.

policy learned from non-expert or script for all metrics.
Results demonstrate the improvement over the efficiency of
policy learning gained from expert human demonstrations.
Apart from the above quantitative results, we further analyze the qualitative results at 50 epochs. We visualize all the
cases in the format of videos, and compare the actions from
policies that were learned from the human (including expert
and non-expert) and the script. A distinct observation has
been found as illustrated in Fig. 6. The first row shows a failure case of policy learned from the script, where the instrument first reaches the needle and tries to pick it up, but fails
to grasp it. After failing on the first attempt, the policy could
not grasp the needle again in the following steps. Instead,
when learning from human demonstrations, the policy can
grasp the needle again and complete the task after failing on
the first failure grasping trail. Results show that using human
demonstrations can help policy more quickly grasp the skills
to handle failure cases at early learning stage, which demonstrate the effectiveness of incorporating the human factor for
policy learning in the form of human demonstration.

V. CONCLUSION AND DISCUSSION
In this paper, we propose an interactive platform based
on our existing SurRoL simulator for human-in-the-loop
embodied intelligence. New features are added for enhanced
human interaction, including haptic interaction interface,
physical simulation and scene rendering with better realism.
Five new representative surgical training tasks are also codeveloped for future work on human-in-the-loop surgical task
learning. We initially conduct basic experiments to validate
the idea of including human in the embodied intelligence
in the form of human demonstration. The promising results
prove the effectiveness of our proposed methods and
pave the way for relevant research topics. Potential future
directions include taking the surgical safety [65, 66] into
consideration for policy learning from human demonstration,
studying the effectiveness of learning from human feedback,
and human-robot collaboration [22]. We envisage extensive
future works to explore how human factors can play a role
and transform surgical robot learning through our provided
open-source embodied AI platform for surgical robots.


REFERENCES

[1] R. H. Taylor, N. Simaan, A. Menciassi, and G.-Z. Yang,
“Surgical robotics and computer-integrated interventional medicine,”
_Proceedings of the IEEE, vol. 110, no. 7, pp. 823–834, 2022._

[2] C. D’Ettorre, A. Mariani, A. Stilli, P. Valdastri, A. Deguet,
P. Kazanzides, R. H. Taylor, G. S. Fischer, S. P. DiMaio, A. Menciassi,
_et al., “Accelerating surgical robotics research: Reviewing 10 years_
of research with the dvrk,” arXiv preprint arXiv:2104.09869, 2021.

[3] G.-Z. Yang, J. Bellingham, et al., “The grand challenges of science
robotics,” Science robotics, vol. 3, no. 14, p. eaar7650, 2018.

[4] L. Maier-Hein, M. Eisenmann, D. Sarikaya, K. M¨arz, T. Collins,
A. Malpani, J. Fallert, H. Feussner, S. Giannarou, P. Mascagni, et al.,
“Surgical data science–from concepts toward clinical translation,”
_Medical image analysis, vol. 76, p. 102306, 2022._

[5] J. Duan, S. Yu, H. L. Tan, H. Zhu, and C. Tan, “A survey of
embodied ai: From simulators to research tasks,” IEEE Transactions
_on Emerging Topics in Computational Intelligence, 2022._

[6] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain,
J. Straub, J. Liu, V. Koltun, J. Malik, et al., “Habitat: A platform for
embodied ai research,” in Proceedings of the IEEE/CVF International
_Conference on Computer Vision, 2019, pp. 9339–9347._

[7] R. Ramrakhya, E. Undersander, D. Batra, and A. Das, “Habitatweb: Learning embodied object-search strategies from human
demonstrations at scale,” in Proceedings of the IEEE/CVF Conference
_on Computer Vision and Pattern Recognition, 2022, pp. 5173–5183._

[8] C. Li, F. Xia, R. Mart´ın-Mart´ın, M. Lingelbach, S. Srivastava,
B. Shen, K. Vainio, C. Gokmen, G. Dharan, T. Jain, et al., “igibson
2.0: Object-centric simulation for robot learning of everyday
household tasks,” arXiv preprint arXiv:2108.03272, 2021.

[9] F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang,
Y. Yuan, H. Wang, L. Yi, A. X. Chang, L. J. Guibas, and H. Su,
“SAPIEN: A simulated part-based interactive environment,” in IEEE
_Conference on Computer Vision and Pattern Recognition, June 2020._

[10] S. Bonne, W. Panitch, K. Dharmarajan, K. Srinivas, J.-L. Kincade,
T. Low, B. Knoth, C. Cowan, D. Fer, B. Thananjeyan, et al., “A
digital twin framework for telesurgery in the presence of varying
network quality of service,” in CASE. IEEE, 2022, pp. 1325–1332.

[11] H. Shu, R. Liang, Z. Li, A. Goodridge, X. Zhang, H. Ding, N. Nagururu, M. Sahu, F. X. Creighton, R. H. Taylor, et al., “Twin-s: A digital
twin for skull-base surgery,” arXiv preprint arXiv:2211.11863, 2022.

[12] J. Ibarz, J. Tan, C. Finn, M. Kalakrishnan, P. Pastor, and S. Levine,
“How to train your robot with deep reinforcement learning: lessons
we have learned,” The International Journal of Robotics Research,
vol. 40, no. 4-5, pp. 698–721, 2021.

[13] E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti,
D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi, “AI2-THOR: An
Interactive 3D Environment for Visual AI,” arXiv, 2017.

[14] P. A. Kenney, M. F. Wszolek, et al., “Face, content, and construct
validity of dv-trainer, a novel virtual reality simulator for robotic
surgery,” Urology, vol. 73, no. 6, pp. 1288–1292, 2009.

[15] G. Whittaker, A. Aydin, N. Raison, F. Kum, B. Challacombe, M. S.
Khan, et al., “Validation of the robotix mentor robotic surgery
simulator,” Journal of endourology, vol. 30, no. 3, pp. 338–346, 2016.

[16] F. Richter et al., “Open-sourced reinforcement learning environments
for surgical robotics,” arXiv preprint arXiv:1903.02090, 2019.

[17] E. Tagliabue, A. Pore, D. Dall’Alba, E. Magnabosco, M. Piccinelli, and
P. Fiorini, “Soft tissue simulation environment to learn manipulation
tasks in autonomous robotic surgery,” in International Conference on
_Intelligent Robots and Systems (IROS)._ IEEE, 2020, pp. 3261–3266.

[18] A. Munawar, J. Y. Wu, et al., “Open simulation environment for
learning and practice of robot-assisted surgical suturing,” IEEE RAL,
vol. 7, no. 2, pp. 3843–3850, 2022.

[19] G.-Z. Yang, J. Cambias, K. Cleary, E. Daimler, J. Drake, P. E.
Dupont, N. Hata, P. Kazanzides, S. Martel, R. V. Patel, et al.,
“Medical robotics—regulatory, ethical, and legal considerations for
increasing levels of autonomy,” p. eaam8638, 2017.


-----

[20] H. Saeidi, J. D. Opfermann, M. Kam, S. Wei, S. L´eonard, Hsieh,
_et al., “Autonomous robotic laparoscopic surgery for intestinal_
anastomosis,” Science Robotics, vol. 7, no. 62, p. eabj2908, 2022.

[21] D. Zhang, Y. Guo, J. Chen, J. Liu, and G.-Z. Yang, “A handheld master
controller for robot-assisted microsurgery,” in International Confer_ence on Intelligent Robots and Systems._ IEEE, 2019, pp. 394–400.

[22] D. Zhang, Z. Wu, J. Chen, R. Zhu, A. Munawar, B. Xiao, et al.,
“Human-robot shared control for surgical robot based on contextaware sim-to-real adaptation,” arXiv preprint arXiv:2204.11116, 2022.

[23] J. Xu, B. Li, B. Lu, Y.-H. Liu, Q. Dou, and P.-A. Heng, “Surrol:
An open-source reinforcement learning centered and dvrk compatible
platform for surgical robot learning,” in IROS. IEEE, 2021.

[24] K. Ehsani, W. Han, A. Herrasti, E. VanderBilt, L. Weihs, E. Kolve,
A. Kembhavi, and R. Mottaghi, “Manipulathor: A framework for
visual object manipulation,” CVPR, pp. 4495–4504, 2021.

[25] C. Gan, J. Schwartz, S. Alter, M. Schrimpf, J. Traer, J. De Freitas,
J. Kubilius, et al., “Threedworld: A platform for interactive multimodal physical simulation,” arXiv preprint arXiv:2007.04954, 2020.

[26] H. Fu, W. Xu, H. Xue, H. Yang, R. Ye, Y. Huang, et al., “Rfuniverse:
A physics-based action-centric interactive environment for everyday
household tasks,” arXiv preprint arXiv:2202.00199, 2022.

[27] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni,
L. Fei-Fei, S. Savarese, Y. Zhu, and R. Mart´ın-Mart´ın, “What
matters in learning from offline human demonstrations for robot
manipulation,” arXiv preprint arXiv:2108.03298, 2021.

[28] K. L. Schwaner, I. Iturrate, J. K. Andersen, P. T. Jensen, and T. R.
Savarimuthu, “Autonomous bi-manual surgical suturing based on
skills learned from demonstration,” in International Conference on
_Intelligent Robots and Systems (IROS)._ IEEE, 2021, pp. 4017–4024.

[29] B. Thananjeyan, A. Garg, S. Krishnan, C. Chen, L. Miller, and
K. Goldberg, “Multilateral surgical pattern cutting in 2d orthotropic
gauze with deep reinforcement learning policies for tensioning,” in
_ICRA._ IEEE, 2017, pp. 2371–2378.

[30] C. D’Ettorre, S. Zirino, N. N. Dei, A. Stilli, E. De Momi, and D. Stoyanov, “Learning intraoperative organ manipulation with context-based
reinforcement learning,” International Journal of Computer Assisted
_Radiology and Surgery, vol. 17, no. 8, pp. 1419–1427, 2022._

[31] J. Hua, L. Zeng, G. Li, and Z. Ju, “Learning for a robot: Deep
reinforcement learning, imitation learning, transfer learning,” Sensors,
vol. 21, no. 4, p. 1278, 2021.

[32] M. Yip and N. Das, “Robot autonomy for surgery,” in The
_Encyclopedia of MEDICAL ROBOTICS: Volume 1 Minimally Invasive_
_Surgical Robotics._ World Scientific, 2019, pp. 281–313.

[33] P. M. Scheikl, E. Tagliabue, B. Gyenes, M. Wagner, D. Dall’Alba,
P. Fiorini, and F. Mathis-Ullrich, “Sim-to-real transfer for visual
reinforcement learning of deformable object manipulation for
robot-assisted surgery,” IEEE RAL, vol. 8, no. 2, pp. 560–567, 2022.

[34] X. Tan, C.-B. Chng, Y. Su, K.-B. Lim, and C.-K. Chui, “Robot-assisted
training in laparoscopy using deep reinforcement learning,” IEEE
_Robotics and Automation Letters, vol. 4, no. 2, pp. 485–492, 2019._

[35] A. Pore, E. Tagliabue, M. Piccinelli, D. Dall’Alba, A. Casals, and
P. Fiorini, “Learning from demonstrations for autonomous soft-tissue
retraction,” in 2021 ISMR. IEEE, 2021, pp. 1–7.

[36] B. Li, R. Wei, J. Xu, B. Lu, et al., “3d perception based imitation
learning under limited demonstration for laparoscope control in
robotic surgery,” in ICRA. IEEE, 2022, pp. 7664–7670.

[37] F. Richter, J. Lu, R. K. Orosco, and M. C. Yip, “Robotic tool tracking
under partially visible kinematic chain: A unified approach,” IEEE
_Transactions on Robotics, vol. 38, no. 3, pp. 1653–1670, 2021._

[38] M. A. Arteaga, A. Guti´errez-Giles, et al., “The geomagic touch haptic
device,” in Local Stability and Ultimate Boundedness in the Control
_of Robot Manipulators._ Springer, 2022, pp. 361–374.

[39] A. Munawar, Z. Li, P. Kunjam, N. Nagururu, A. S. Ding,
P. Kazanzides, T. Looi, F. X. Creighton, R. H. Taylor, and M. Unberath, “Virtual reality for synergistic surgical training and data
generation,” Computer Methods in Biomechanics and Biomedical En_gineering: Imaging & Visualization, vol. 10, no. 4, pp. 366–374, 2022._

[40] Simulated Surgical Systems, “Robotic Surgery Simulator (RoSS[™]),”
[2020, http://simulatedsurgicals.com/projects/ross/.](http://simulatedsurgicals.com/projects/ross/)

[41] K. H. Ang, G. Chong, and Y. Li, “Pid control system analysis, design,
and technology,” IEEE transactions on control systems technology,
vol. 13, no. 4, pp. 559–576, 2005.

[42] D. M. Beazley et al., “Swig: An easy to use tool for integrating
scripting languages with c and c++.” in Tcl/Tk Workshop, 1996.

[43] K. Mamou, E. Lengyel, and A. Peters, “Volumetric hierarchical


approximate convex decomposition,” in Game Engine Gems 3. AK
Peters, 2016, pp. 141–158.

[44] M. Goslin and M. R. Mine, “The panda3d graphics engine,”
_Computer, vol. 37, no. 10, pp. 112–114, 2004._

[45] G. Bishop and D. M. Weimer, “Fast phong shading,” ACM SIGGRAPH
_Computer Graphics, vol. 20, no. 4, pp. 103–106, 1986._

[46] H. Gouraud, “Continuous shading of curved surfaces,” _IEEE_
_transactions on computers, vol. 100, no. 6, pp. 623–629, 1971._

[47] Y. Long, J. Cao, A. Deguet, R. H. Taylor, and Q. Dou, “Integrating
artificial intelligence and augmented reality in robotic surgery: An
initial dvrk study using a surgical education scenario,” in 2022
_International Symposium on Medical Robotics (ISMR), 2022, pp. 1–8._

[48] R. Smith, V. Patel, and R. Satava, “Fundamentals of robotic
surgery: a course of basic robotic surgery skills based upon a
14-society consensus template of outcomes measures and curriculum
development,” The international journal of medical robotics and
_computer assisted surgery, vol. 10, no. 3, pp. 379–384, 2014._

[49] D. Sanford, R. Ma, A. Ghoreifi, et al., “Association of suturing
technical skill assessment scores between virtual reality simulation
and live surgery,” Journal of Endourology, no. ja, 2022.

[50] C. Perrenot, M. Perez, N. Tran, J.-P. Jehl, J. Felblinger, L. Bresler,
and J. Hubert, “The virtual reality simulator dv-trainer® is a valid
assessment tool for robotic surgical skills,” Surgical endoscopy,
vol. 26, no. 9, pp. 2587–2593, 2012.

[51] A. Cowan, J. Chen, S. Mingo, S. S. Reddy, R. Ma, et al., “Virtual
reality vs dry laboratory models: Comparing automated performance
metrics and cognitive workload during robotic simulation training,”
_Journal of Endourology, vol. 35, no. 10, pp. 1571–1576, 2021._

[52] S. Levine and V. Koltun, “Guided policy search,” in International
_conference on machine learning._ PMLR, 2013, pp. 1–9.

[53] M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot,
N. Heess, T. Roth¨orl, T. Lampe, and M. Riedmiller, “Leveraging
demonstrations for deep reinforcement learning on robotics problems
with sparse rewards,” arXiv preprint arXiv:1707.08817, 2017.

[54] H. Su, A. Mariani, S. E. Ovur, A. Menciassi, G. Ferrigno, and
E. De Momi, “Toward teaching by demonstration for robot-assisted
minimally invasive surgery,” TASE, vol. 18, no. 2, pp. 484–494, 2021.

[55] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep
reinforcement learning,” in ICLR, 2016.

[56] V. G. Goecks, G. M. Gremillion, V. J. Lawhern, J. Valasek, and N. R.
Waytowich, “Integrating behavior cloning and reinforcement learning
for improved performance in dense and sparse reward environments,”
in Proceedings of the 19th International Conference on Autonomous
_Agents and MultiAgent Systems, 2020, pp. 465–473._

[57] M. Bain and C. Sammut, “A framework for behavioural cloning.” in
_Machine Intelligence 15, 1995, pp. 103–129._

[58] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong,
P. Welinder, B. McGrew, J. Tobin, O. P. Abbeel, and W. Zaremba,
“Hindsight experience replay,” in NeurIPS, 2017.

[59] T. Huang, K. Chen, B. Li, Y.-H. Liu, and Q. Dou, “Demonstrationguided reinforcement learning with efficient exploration for task
automation of surgical robot,” IEEE ICRA, 2023.

[60] P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford,
J. Schulman, S. Sidor, Y. Wu, and P. Zhokhov, “Openai baselines,”
[https://github.com/openai/baselines, 2017.](https://github.com/openai/baselines)

[61] R. M. Satava, “The future of surgical simulation,” Comprehensive
_Healthcare Simulation: Surgery and Surgical Subspecialties, 2019._

[62] A. G. Gallagher et al., Fundamentals of surgical simulation:
_principles and practice._ Springer Science & Business Media, 2011.

[63] M. Laskin, A. Srinivas, and P. Abbeel, “Curl: Contrastive unsupervised
representations for reinforcement learning,” in _International_
_Conference on Machine Learning._ PMLR, 2020, pp. 5639–5650.

[64] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas,
“Reinforcement learning with augmented data,” Advances in neural
_information processing systems, vol. 33, pp. 19 884–19 895, 2020._

[65] B. Thananjeyan, A. Balakrishna, U. Rosolia, F. Li, R. McAllister,
J. E. Gonzalez, S. Levine, F. Borrelli, and K. Goldberg, “Safety
augmented value estimation from demonstrations (saved): Safe deep
model-based rl for sparse cost robotic tasks,” IEEE Robotics and
_Automation Letters, vol. 5, no. 2, pp. 3612–3619, 2020._

[66] A. Pore, D. Corsi, E. Marchesini, D. Dall’Alba, A. Casals, A. Farinelli,
and P. Fiorini, “Safe reinforcement learning using formal verification
for tissue retraction in autonomous robotic-assisted surgery,” in IROS.
IEEE, 2021, pp. 4025–4031.


-----

