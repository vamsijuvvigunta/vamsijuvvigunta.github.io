{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize the Arxiv papers\n",
    "\n",
    " - ‚òëÔ∏è Search for `cs.AI`\n",
    " - ‚òëÔ∏è Download PDF. Limit to 10/day for last 3 years _(so far have 3/day done as downloads are taking a very long time)_\n",
    " - ‚òëÔ∏è Convert PDF to Markdown text \n",
    " - ‚úîÔ∏è Send to gpt-4o-mini for high quality summary\n",
    " - ‚úîÔ∏è Send it via their paralell batch script that takes care of request/token rate limits and retries\n",
    " - ‚úîÔ∏è Deal with some problem/exception related to disallowed tokens coming through.\n",
    " - ‚úîÔ∏è Add ID to the Markdown source so the batch result can be tied bakc to it's source.\n",
    "\n",
    "## Performance gotchas\n",
    "\n",
    " - Running them serially. `gpt-4o-mini` takes 6 minutes for 30 summarizations _(I have 3000 to go though)_. This is roughly 12s per.\n",
    " - Going to see if paralell invocation while staying under the rate-limit is possible.\n",
    "   - Had to fix their code to add `allowed_special` to the encoder\n",
    "   - Send tokenizer name specific to gpt-4o-mini\n",
    "   - Constantly running into TPM limits but it keeps chugging along\n",
    "\n",
    "Gemini flash has much higher token and request limits, however, there I do not have readymade paralell-call code. I did find some examples in the cookbook but there was no facility to handle backoffs and retries. Don't want to mess with that yet when I have a working OpenAI setup. Took a 30-file copy so I can continue to work on the topic-extraction and UI part of it while the full 3k batch complete on OpenAI: I am expecting it to tak 5 hours (9pm completion).\n",
    "\n",
    "## Prompt for summary\n",
    "\n",
    "There are many examples\n",
    "\n",
    "### Trivial prompts\n",
    "\n",
    "   - give me the TLDR\n",
    "   - summarize the document\n",
    "\n",
    "### Comprehensive prompts\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "### ChatGPT generated\n",
    "\n",
    "\n",
    "<pre>\n",
    "Generate a prompt I can use to summarize technical arxiv papers.  The papers are technical in nature and contain \n",
    "AI subject matter. I want the summaries to be long enough to be used successfully in topic modeling. \n",
    "</pre>\n",
    "\n",
    "‚Ü™\n",
    "\n",
    "<pre>\n",
    "Prompt:\n",
    "\n",
    "\"Summarize the following arXiv paper in a detailed and structured manner, focusing on its key contributions, \n",
    "methodology, experimental results, and conclusions. The summary should be long enough (approximately \n",
    "300-500 words) to capture important technical details for topic modeling purposes.\n",
    "\n",
    "Ensure that the summary includes:\n",
    "\n",
    " - Title and Authors ‚Äì Include the paper‚Äôs title and author names.\n",
    " - Abstract Summary ‚Äì Provide a high-level summary of the abstract.\n",
    " - Problem Statement ‚Äì Clearly describe the research problem being addressed and its significance.\n",
    " - Methodology ‚Äì Summarize the approach, techniques, and models used, including any mathematical \n",
    "   formulations if applicable.\n",
    " - Experiments and Results ‚Äì Describe the datasets used, evaluation metrics, and key findings, \n",
    "   including tables or numerical results if relevant.\n",
    " - Conclusions and Implications ‚Äì Explain the broader impact of the findings and possible \n",
    "   future directions.\n",
    " - Keywords and Topics ‚Äì Extract key technical terms and concepts for topic modeling.\n",
    " - Ensure clarity and coherence while maintaining technical precision. The summary \n",
    "   should avoid excessive jargon where possible while still being informative to researchers \n",
    "   familiar with AI and machine learning topics.\"\n",
    "</pre>\n",
    "\n",
    "### Other resources\n",
    " - Fabric!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths to our libs\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "lib_path = (Path(os.getcwd()) / \"lib\").resolve()\n",
    "sys.path.append(str(lib_path))\n",
    "\n",
    "# Import jupyter utils\n",
    "import logging\n",
    "from util import jupyter_util\n",
    "from util.jupyter_util import DisplayHTML as jh\n",
    "from util.jupyter_util import DisplayMarkdown as jm\n",
    "\n",
    "# Init jupyter env. Set to DEBUG if you want to see the gory details\n",
    "# of schemas and such.\n",
    "jupyter_util.setup_logging(logging.WARNING)\n",
    "jupyter_util.ColabEnv.import_api_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DATA_DIR = Path(os.getcwd()) / \"data\"\n",
    "FEED_RAW_DATA_DIR = DATA_DIR / \"feed\" / \"raw\"\n",
    "\n",
    "# Test with the small 3 day one first\n",
    "#metadata_path = FEED_RAW_DATA_DIR / \"Arxiv_csAI_API_dailysampled_3d.json\"\n",
    "metadata_path = FEED_RAW_DATA_DIR / \"Arxiv_csAI_API_dailysampled_3y.json\"\n",
    "\n",
    "#----------------------------\n",
    "# Read the JSON in\n",
    "arxiv_per_day_mtd = {}\n",
    "with open( str(metadata_path), 'r') as json_data:\n",
    "    arxiv_per_day_mtd = json.load(json_data)\n",
    "\n",
    "logging.debug(f\"Finished loading JSON from {str(metadata_path)}.\\nHave {len(arxiv_per_day_mtd)} days worth of records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested by ChatGPT\n",
    "# Thought I could use JSON but even if it sends it in JSON, some of the items\n",
    "# are not legal JSON. Don't want to chase this. Down. Will just use a Markdown parser to extract the \n",
    "# topics and keywords separately\n",
    "summarization_prompt = \"\"\"\n",
    "Summarize the following arXiv paper in a detailed and structured manner, focusing on its key contributions, \n",
    "methodology, experimental results, and conclusions. The summary should be long enough (approximately \n",
    "300-500 words) to capture important technical details for topic modeling purposes.\n",
    "\n",
    "Ensure that the summary includes:\n",
    "\n",
    " - Title and Authors ‚Äì Include the paper‚Äôs title and author names.\n",
    " - Abstract Summary ‚Äì Provide a high-level summary of the abstract.\n",
    " - Problem Statement ‚Äì Clearly describe the research problem being addressed and its significance.\n",
    " - Methodology ‚Äì Summarize the approach, techniques, and models used, including any mathematical formulations if applicable.\n",
    " - Experiments and Results ‚Äì Describe the datasets used, evaluation metrics, and key findings, including tables or numerical results if relevant.\n",
    " - Conclusions and Implications ‚Äì Explain the broader impact of the findings and possible future directions.\n",
    " - Keywords and Topics ‚Äì Extract key technical terms and concepts for topic modeling.\n",
    "\n",
    "Ensure clarity and coherence while maintaining technical precision. The summary should avoid excessive jargon where possible while still being informative to researchers familiar with AI and machine learning topics.\n",
    "\n",
    "The paper is included after the separator as a markdown document.\n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from llm.openai_util import get_completion\n",
    "from dateutil import parser\n",
    "\n",
    "PAPER_PDF_DIR     = DATA_DIR / \"arxiv\" / \"pdf\"\n",
    "PAPER_SUMMARY_DIR = DATA_DIR / \"arxiv\" / \"summary_md\"\n",
    "BULK_REQ_DIR      = Path(os.getcwd()) / \"oai_bulk_req\"\n",
    "\n",
    "def serially_save_summary(per_day_dict):\n",
    "    # Loop through and build the path of the extract Markdown files\n",
    "    for day_str, day_items in per_day_dict.items():\n",
    "        for day_item in day_items:\n",
    "            pdf_url      = day_item[\"pdf_url\"]\n",
    "            day_dir      = parser.parse(day_str).strftime(\"%m_%d_%Y\")\n",
    "            fname_wo_ext = pdf_url.split(\"/\")[-1]\n",
    "            pdf_path     = str(PAPER_PDF_DIR / day_dir / f\"{fname_wo_ext}.pdf\")\n",
    "            summ_path    = str(PAPER_SUMMARY_DIR / day_dir / f\"{fname_wo_ext}.md\")\n",
    "            md_path      = pdf_path.replace(\"pdf\", \"md\")\n",
    "            #print(md_path)\n",
    "\n",
    "            # Ensure output dir exists\n",
    "            os.makedirs(str(PAPER_SUMMARY_DIR / day_dir), exist_ok=True)\n",
    "\n",
    "            # Convert\n",
    "            md_text = Path(md_path).read_text()\n",
    "            summary = get_completion(f\"{summarization_prompt}\\n{md_text}\")\n",
    "\n",
    "            # Save to \n",
    "            Path(summ_path).write_text(summary)\n",
    "            logging.debug(f\"‚úîÔ∏è - {summ_path}\")\n",
    "\n",
    "def gen_batch_requests_for_summary(per_day_dict, req_file_path):    \n",
    "\n",
    "    num_req = 0\n",
    "    with open(req_file_path, \"w\") as f:\n",
    "\n",
    "        for day_str, day_items in per_day_dict.items():\n",
    "\n",
    "            day_dir      = parser.parse(day_str).strftime(\"%m_%d_%Y\")\n",
    "\n",
    "            for day_item in day_items:\n",
    "                pdf_url      = day_item[\"pdf_url\"]        \n",
    "                fname_wo_ext = pdf_url.split(\"/\")[-1]\n",
    "                pdf_fpath     = str(PAPER_PDF_DIR / day_dir / f\"{fname_wo_ext}.pdf\")            \n",
    "                md_fpath      = pdf_fpath.replace(\"pdf\", \"md\")           \n",
    "                summ_path     = md_fpath.replace(\"/md/\", \"/summary/\")\n",
    "                \n",
    "                # If summary has already been created, don't add to job.\n",
    "                if Path(summ_path).exists():\n",
    "                    logging.debug(f\"Summary file {summ_path} is already present. Not adding to creation job\")\n",
    "                    continue\n",
    "\n",
    "                # Convert                \n",
    "                md_path = Path(md_fpath)\n",
    "                if not md_path.exists():\n",
    "                    # We have some errors will the PDF itself missing from the server\n",
    "                    # and/or the PDF conversion to MD failing\n",
    "                    continue\n",
    "\n",
    "                # üëâ I also embed a \n",
    "                #   $$MD_PATH$$:{md_fpath}\n",
    "                # to the end so I can later parse it out and know which MD\n",
    "                # it came from and tie it back in post\n",
    "                md_text = Path(md_path).read_text()                \n",
    "                job = {\n",
    "                    \"model\"   : \"gpt-4o-mini\", \n",
    "                    \"temperature\" : 1,\n",
    "                    \"messages\": [{\n",
    "                        \"role\" : \"user\",\n",
    "                        \"content\" : f\"{summarization_prompt}\\n{md_text}\\n$$MD_PATH$$:{md_fpath}\"\n",
    "                }]}\n",
    "\n",
    "                json_string = json.dumps(job)\n",
    "                num_req+= 1\n",
    "                f.write(json_string + \"\\n\")\n",
    "    \n",
    "    logging.debug(\"Wrote {num_req} OpenAI jobs\")\n",
    "\n",
    "\n",
    "# Generate the bulk requests.\n",
    "# Run these as follows (see Scratchpad below)\n",
    "#\n",
    "# > cd oai_bulk_req\n",
    "# > python ../bin/api_request_parallel_processor.py --token_encoding_name o200k_base --requests_filepath 3d_summarization_req.jsonl --save_filepath 3d_summarizations.jsonl --request_url https://api.openai.com/v1/chat/completions --max_requests_per_minute 500 --max_tokens_per_minute 200000\n",
    "gen_batch_requests_for_summary(arxiv_per_day_mtd, \n",
    "                               #str(BULK_REQ_DIR / \"3d_summarization_req.jsonl\")\n",
    "                               str(BULK_REQ_DIR / \"3y_summarization_req.jsonl\")\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The batch execution process dies once in a while. Takes forever so I want to \n",
    "# save the outputs so that the batch-request process only generates reqs \n",
    "# for the remaining files\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the generated summaries\n",
    "# While this is jsonl, the format is strange\n",
    "# Each line is an array of 2 items\n",
    "#   [ {job}, {result}]\n",
    "# So shows up as two unnamed columns in pandas\n",
    "GENERATED_SUMMARIES_FILE = BULK_REQ_DIR / \"3y_summarizations.jsonl\"\n",
    "summary_gen_jsonl_df = pd.read_json(str(GENERATED_SUMMARIES_FILE), lines=True)\n",
    "\n",
    "for index, row in summary_gen_jsonl_df.iterrows():\n",
    "    # Get the prompt\n",
    "    # It has an embedded $$MD_PATH$$:{md_fpath} at the end\n",
    "    # row[0]: json\n",
    "    #    model\n",
    "    #    temperature\n",
    "    #    messages : [\n",
    "    #         role :\n",
    "    #         content:\n",
    "    #    ]\n",
    "    prompt = row[0][\"messages\"][0][\"content\"]\n",
    "    m = re.findall(r'^\\$\\$MD_PATH\\$\\$:(.*?)$',  prompt, re.MULTILINE)\n",
    "    if m and len(m) > 0:\n",
    "        assert(len(m) == 1)\n",
    "        md_fpath = m[0]\n",
    "        sum_path = md_fpath.replace('/md/', '/summary/')\n",
    "\n",
    "        # -- row[1]\n",
    "        # id : \n",
    "        # object: \"chat.completion\"\n",
    "        # created: \n",
    "        # model:\n",
    "        # choices: [{\n",
    "        #     index: \n",
    "        #     message: {\n",
    "        #         role: \"assistant\"\n",
    "        #         content: üëâ this is the summary \n",
    "        #         refusal : \n",
    "        #         annotations :\n",
    "        #     }\n",
    "        # }]    \n",
    "        choices = row[1][\"choices\"]    \n",
    "        if len(choices) > 0:\n",
    "            assert(len(choices) == 1)\n",
    "            choice_msg = choices[0][\"message\"]        \n",
    "            if choice_msg[\"refusal\"]:\n",
    "                logging.warning(f\"Response was refucsed by OpenAI: {choice_msg[\"refusal\"]}\")\n",
    "            else:\n",
    "                sum_file = Path(sum_path)\n",
    "                if sum_file.exists():\n",
    "                    logging.warning(f\"Summary file already exists: Not creating: {sum_path}\")\n",
    "                else:\n",
    "                    logging.debug(f\"Writing {sum_path}\")\n",
    "                    os.makedirs(os.path.dirname(sum_file), exist_ok=True)                    \n",
    "                    sum_file.write_text(choice_msg[\"content\"])\n",
    "        else:\n",
    "            logging.warning(\"Response is missing for request ??\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/vamsi/bitbucket/hillops/nbs/BSL_TakeHome/data/arxiv/md/03_16_2025/2503.12688v1.md']\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "blah blah blah\n",
    "$$MD_PATH$$:/home/vamsi/bitbucket/hillops/nbs/BSL_TakeHome/data/arxiv/md/03_16_2025/2503.12688v1.md\n",
    "\"\"\".strip()\n",
    "\n",
    "import re\n",
    "m = re.findall(r'^\\$\\$MD_PATH\\$\\$:(.*?)$',  txt, re.MULTILINE)\n",
    "print(m)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad - Test OpenAI paralell calls\n",
    "\n",
    " - See https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py\n",
    " - Takes requests from a `jsonl` file where each row _(a complete json block)_ is a request\n",
    " - Their code to generate the req is listed below. Will modify it to tell a joke and make 10 requests and see\n",
    "\n",
    " - https://api.openai.com/v1/chat/completions\n",
    " - https://platform.openai.com/settings/organization/limits\n",
    "   - tokens = 200,000 TPM\n",
    "   - 500 RPM, 10,000 RPD\n",
    "\n",
    " - Generated the jsonl file using the cell below\n",
    " - Downloaded the file from https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py \n",
    " - Used the following command to run it.\n",
    "\n",
    "```bash\n",
    "(jupyter) vamsi@hillops_dev:~/bitbucket/hillops/nbs/BSL_TakeHome/oai_bulk_req$python ../bin/api_request_parallel_processor.py --requests_filepath jokes_paralell.jsonl --save_filepath jokes.jsonl --request_url https://api.openai.com/v1/chat/completions --max_requests_per_minute 500 --max_tokens_per_minute 200000\n",
    "```   \n",
    "\n",
    "**Problems**\n",
    " - When running the actual summarization, it gave me an error about encoding `<|endoftext|>` and asked me to set `disallowed_tokens=()`. Seems to be a tokenizer setting. I replaced the default `  ` with the one meant for gpt-4o-mini: `o200k_base` via `--token_encoder_base o200k_base` and re-ran the batch. Still failed!\n",
    "\n",
    "```diff\n",
    "def num_tokens_consumed_from_request(\n",
    "    request_json: dict,\n",
    "    api_endpoint: str,\n",
    "    token_encoding_name: str,\n",
    "):\n",
    "    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(token_encoding_name)\n",
    "    # if completions request, tokens = prompt + n * max_tokens\n",
    "    if api_endpoint.endswith(\"completions\"):\n",
    "        max_tokens = request_json.get(\"max_tokens\", 15)\n",
    "        n = request_json.get(\"n\", 1)\n",
    "        completion_tokens = n * max_tokens\n",
    "\n",
    "        # chat completions\n",
    "        if api_endpoint.startswith(\"chat/\"):\n",
    "            num_tokens = 0\n",
    "            for message in request_json[\"messages\"]:\n",
    "                num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "                for key, value in message.items():\n",
    "-                   num_tokens += len(encoding.encode(value))\n",
    "+                   num_tokens += len(encoding.encode(value, allowed_special=\"all\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "BULK_REQ_DIR = Path(os.getcwd()) / \"oai_bulk_req\"\n",
    "\n",
    "filename = BULK_REQ_DIR / \"jokes_paralell.jsonl\"\n",
    "n_requests = 10\n",
    "jobs = [\n",
    "    {\"model\"   : \"gpt-4o-mini\", \n",
    "     \"temperature\" : 1,\n",
    "     \"messages\": [{\n",
    "         \"role\" : \"user\",\n",
    "         \"content\" : \"Tell me a joke\"         \n",
    "         }],\n",
    "    } for x in range(n_requests)]\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for job in jobs:\n",
    "        json_string = json.dumps(job)\n",
    "        f.write(json_string + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "end = tiktoken.get_encoding(\"o200k_base\", allowed_special={'<|endoftext|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
